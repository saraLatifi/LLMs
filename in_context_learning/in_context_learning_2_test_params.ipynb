{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Test some parameters**"
      ],
      "metadata": {
        "id": "DTurDTueFxeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozz3rCFeFsQP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "40-kUSjfF1BL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(hf_dataset_name)"
      ],
      "metadata": {
        "id": "UhhPVOfpF2uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "c9WB7raIF2rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "  prompt = ''\n",
        "\n",
        "  for index in example_indices_full:\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5.\n",
        "    # Other models may have their own preferred stop sequence.\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "summary:\n",
        "{summary}\\n\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "  dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summarize:\n",
        "\"\"\"\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "bk6cOtnhF4tY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)"
      ],
      "metadata": {
        "id": "zmX7HcAfF9Xm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **As Baseline**"
      ],
      "metadata": {
        "id": "Cr6hJT5YRQao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = dataset['test'][200]['dialogue']\n",
        "\n",
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "WzR0JJCHF_FA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Dialogue: \\n{dialogue}\\n')\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlrDW-AtW7Kp",
        "outputId": "5b4abffe-0160-4409-9812-9f4d32c6bea1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue: \n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Generated Summary: #Person1 wants to upgrade his computer. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **max_new_tokens**\n"
      ],
      "metadata": {
        "id": "1S2r0-PjGCDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=10,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQku5ZvTGDFA",
        "outputId": "ca83b852-5008-484f-9e12-c7503e2a7cd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: #Person1 wants to upgrade his computer.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **do_sample**\n",
        "\n",
        "Default values:\n",
        "\n",
        "```\n",
        "top_k=50  \n",
        "top_p=1.0\n",
        "```"
      ],
      "metadata": {
        "id": "Zz20le8yGHfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True       # turn on sampling\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtVPrWvbGM4m",
        "outputId": "568c6a7b-5c14-4284-9d1e-9f184e6fcf5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Most software comes out on CDs nowadays.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **do_sample** + **top_k** & **top_p**\n",
        "\n",
        "Note:\n",
        "\n",
        "```\n",
        "top_k=0    # disables top-k\n",
        "top_p=1.0  # disables top-p\n",
        "```\n"
      ],
      "metadata": {
        "id": "bvmYVklCGQgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,       # turn on sampling\n",
        "        top_k=20,\n",
        "        top_p=0.90\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbQgPDWrGSJ0",
        "outputId": "a3710bfa-2efe-418d-ab17-645d295db5ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: People are considering upgrading their computers to meet modern computer needs.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **temperature** (& **do_sample**)\n",
        "\n",
        "`temperature` **MUST** be used with `do_sample=True`\n",
        "\n",
        "\n",
        "\n",
        "*   temperature < 1 → less random\n",
        "*   temperature > 1 → more random\n",
        "\n",
        "Typical values:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   temperature → 0 → approximates **greedy** decoding (picks the highest-probability token).\n",
        "*   0 < temperature < 1 → makes output more deterministic (less random).\n",
        "*   temperature = 1 → **default, normal sampling**.\n",
        "*   temperature > 1 → makes output more random (more diverse).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWaC5hDuGWdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,       # turn on sampling\n",
        "        temperature=0.5\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icPyRswFGX1I",
        "outputId": "bad61f79-949b-4358-a343-9a3e0670ee53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: #Person2 will consider upgrading the system that he uses.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    }
  ]
}