{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarization (using a Seq2Seq LLM)**\n",
        "\n",
        "The first step towards prompt engineering and see how it can enhance the generative output of Large Language Models by comparing **zero shot**, **one shot**, and **few shot** inferences.\n"
      ],
      "metadata": {
        "id": "RGH6AMcQG-Kp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z5biJM9H0Gpg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets, Large Language Model (LLM), tokenizer, and configurator."
      ],
      "metadata": {
        "id": "mA2zyovu14Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "k4_ie_G812wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(hf_dataset_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CJrHxaB83Mju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset # 'summary' is human summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DawkdnIR3mLU",
        "outputId": "a5cab159-ccd7-42a1-a27a-c3becaa2be55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "# AutoModel: load the model by its name\n",
        "# ForSeq2Seq: for seq_to_seq task\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "f3L9I3il4lxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models.\n",
        "\n",
        "`AutoTokenizer.from_pretrained()` method downloads the **tokenizer** for the **corresponding model**. Parameter `use_fast` switches on fast tokenizer."
      ],
      "metadata": {
        "id": "oR9krZPe7IrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "771b93541eaf46c18beafc0a89e47b9a",
            "985b26b6e42b422cb1127a97992ea2fd",
            "b9041350a182476ab93e0c8cf8a2962f",
            "aac9101a11424ef5a6193c0f2733328f",
            "94e912f75b7845a2a275665b68179821",
            "ef5781db23c24abf84c9f889ab0d2c4f",
            "401002c590a448c3bed4c0d62c62e5df",
            "e6955a2aabc04110ae7463625e308629",
            "4d4be9e8c490488185f548533c149297",
            "274d124a368d4093b3334c0f8b839689",
            "af817f6ff67545fd9031559b2dec163d",
            "49a69aa412ee4a4d8505ff7b6725f5f1",
            "df53b661c565475e8cd4d77117385330",
            "dadc4736748545d69530cd3817c2e535",
            "fd6842fe00fa4bf482ac965114aa6407",
            "e3d0680bbf51418b870a5a27dc9a56b6",
            "ef7cc4d3e1574cbca558f1335f38be4c",
            "463d84594541444fa151329628b01039",
            "5f1e424e60c04447a95a407af4e4e156",
            "13ff496a2f924c8e87d1a450e521207d",
            "8256340d3ad24659964eb79f478ea1c9",
            "f1dad9db87e446fba2cb49b2a4cd08e9",
            "6b89899d1e99495f9ee2122a30d0117e",
            "697bdc3d211d491cb1603f5c3fab1ac5",
            "886615bb70ca4c178fd02d87b79e89fc",
            "574b3dfadde54c208a08696eb72e2029",
            "41888db241f047da850e82929593c2b1",
            "6dbb149e13c849bb9da29c3670927bda",
            "7d03213f0ac54ebd87455d09662d4d25",
            "0934c2001a6d49b4aa181f4f9b48ca91",
            "36e50d77c42143b18aa53c5c631a9542",
            "66c4506588d44251a5a0e6caedbe3f76",
            "0a044880039a4b0d845c06033e98c33d",
            "9e56e5d71a3b402d8fe73f28810898fe",
            "fde4b8e5909b49708f2aef0973e153af",
            "10a93bad0d7543f59dca526d019975c6",
            "021211f10f5a47bfa821bf785a278bf5",
            "72a0f53e780b412fab09bb2e3ef349ee",
            "74a053434a5544d7acdf87c432a28a67",
            "760f2bb0fd844f9a899151f3b4759181",
            "dfb2bda14fbf405391e7b7e7948bbf81",
            "485ac2ff12394510bc90c5f7e9019f92",
            "e5ae1689ec6a401db236d4db168116f5",
            "6e8510e3a7e849f5a2b7c287ce7156b0"
          ]
        },
        "id": "KPmd8M7n6ymS",
        "outputId": "72d0ccc5-e8a6-4771-e16b-042fe63faf11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "771b93541eaf46c18beafc0a89e47b9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a69aa412ee4a4d8505ff7b6725f5f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b89899d1e99495f9ee2122a30d0117e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e56e5d71a3b402d8fe73f28810898fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the tokenizer **encoding** and **decoding** a simple sentence.\n",
        "\n",
        "\n",
        "The output of `tokenizer` is a **dictionary** containing tensors needed for a model: `input_ids` and `attention_mask`.\n",
        "\n",
        "**input_ids**\n",
        "\n",
        "- *Type*: **torch.Tensor** (because return_tensors='pt')\n",
        "- *Contents*: A sequence of **token IDs** corresponding to your prompt.\n",
        "- *Purpose*: These IDs are the **actual numerical representations** the model uses as input.\n",
        "\n",
        "`prompt (text)  → tokenizer → input_ids (numbers) → model`"
      ],
      "metadata": {
        "id": "7Dvc_4aT8hct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Learning Large Language Models (LLMs) is amazing!\"\n",
        "\n",
        "# return_tensors: define the type of the data structure\n",
        "# pt: pyTorch (& tf: tensorflow)\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "print(f'ENCODED: {sentence_encoded}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUpugXMi8l0u",
        "outputId": "01f7001d-c411-4365-beb8-86f1abdfefa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCODED: {'input_ids': tensor([[ 6630,  7199, 10509,  5154,     7,    41, 10376,   329,     7,    61,\n",
            "            19,  1237,    55,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ENCODED: {sentence_encoded[\"input_ids\"][0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzCLk1Q1-CpL",
        "outputId": "111647b1-2396-432e-cc64-1692316cd48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCODED: tensor([ 6630,  7199, 10509,  5154,     7,    41, 10376,   329,     7,    61,\n",
            "           19,  1237,    55,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`skip_special_tokens=True` removes **special** tokens (e.g., **</s>** which is the *end of sentence* token).\n"
      ],
      "metadata": {
        "id": "DdHGR7RU-yDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print(f'DECODED: {sentence_decoded}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11VOcM_9vR0",
        "outputId": "14d694d4-34a4-4d71-9c21-051d9a8218fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DECODED: Learning Large Language Models (LLMs) is amazing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0-Summarization without Prompt Engineering**\n",
        "\n",
        "**Prompt engineering** is an act of a human changing the prompt to improve the response for a given task."
      ],
      "metadata": {
        "id": "p9VUPItd_mxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test on a dialogue (index=200) in the test set\n",
        "dialogue = dataset['test'][200]['dialogue']\n",
        "summary = dataset['test'][200]['summary']\n",
        "\n",
        "inputs = tokenizer(dialogue, return_tensors='pt')\n",
        "\n",
        "# try to get a summary JUST by limmiting the max_new_tokens\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Dialogue: \\n{dialogue}\\n')\n",
        "\n",
        "# the model was not asked for summarization => the output will no be a summary!\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XOAvYBd_mf-",
        "outputId": "7b411f81-b92b-4427-bfe2-c421c5d2eae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue: \n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Generated Summary: #Person1#: I'm thinking of upgrading my computer.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1-Summarization with an Instruction Prompt**"
      ],
      "metadata": {
        "id": "U3PcpC7nDi5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt engineering** is an important concept in using foundation models for text generation."
      ],
      "metadata": {
        "id": "ebGprSwpEGR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1-Zero Shot Inference with an Instruction Prompt\n",
        "\n",
        "Wrap the dialogue in a descriptive instruction."
      ],
      "metadata": {
        "id": "wz_p8vMSEMVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tokenizer.tgt_lang = \"de\"   # target language = German\n",
        "tokenizer.src_lang = \"en\"   # source language = English\n",
        "\n",
        "tokenizer.tgt_lang is typically an attribute used in tokenizers for multilingual machine translation models (like mBART, M2M100, NLLB, etc.) to specify the target language you want the model to generate.\n",
        "\n",
        "It’s not a universal tokenizer attribute — it appears only in certain models where you must set a target language ID before generation.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "w8zGHXp_PrkK",
        "outputId": "49a50b8f-a1de-4bd8-a441-5ccd10fc37ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntokenizer.tgt_lang = \"de\"   # target language = German\\ntokenizer.src_lang = \"en\"   # source language = English\\n\\ntokenizer.tgt_lang is typically an attribute used in tokenizers for multilingual machine translation models (like mBART, M2M100, NLLB, etc.) to specify the target language you want the model to generate.\\n\\nIt’s not a universal tokenizer attribute — it appears only in certain models where you must set a target language ID before generation.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1.1-Prompt 1**"
      ],
      "metadata": {
        "id": "-BPzL8xSWnkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "# Input constructed prompt (instead of the dialogue)\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "  model.generate(\n",
        "      inputs[\"input_ids\"],\n",
        "      max_new_tokens=50,\n",
        "  )[0],\n",
        "  skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Dialogue: {dialogue}\\n')\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDBzpazRDo0g",
        "outputId": "78e12608-3a72-409f-f14c-9858fa277396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue: #Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Generated Summary: #Person1#: I'm thinking of upgrading my computer.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1.2-Prompt 2**"
      ],
      "metadata": {
        "id": "uQVsPV7UWtoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "    \"\"\"\n",
        "\n",
        "# Input constructed prompt (instead of the dialogue)\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "  model.generate(\n",
        "      inputs[\"input_ids\"],\n",
        "      max_new_tokens=50,\n",
        "  )[0],\n",
        "  skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Dialogue: {dialogue}\\n')\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsHw14mbWzKN",
        "outputId": "784d1af2-9601-4a54-ed10-67676bdc296b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue: #Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Generated Summary: #Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2-One Shot Inference with an Instruction Prompt"
      ],
      "metadata": {
        "id": "MNDG-2tfYJXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "  prompt = ''\n",
        "\n",
        "  for index in example_indices_full:\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5.\n",
        "    # Other models may have their own preferred stop sequence.\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "{summary}\\n\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "  dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "\"\"\"\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "Zpgz_X0cY10Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the prompt to perform one shot inference:"
      ],
      "metadata": {
        "id": "HYCkLHdnbgwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [400] # use as the example\n",
        "example_index_to_summarize = 200 # use to test the model\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SXQoJWcZd-P",
        "outputId": "49c9e972-9ec3-40e9-e0f7-9378940a0aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: It was a heavy storm last night, wasn't it?\n",
            "#Person2#: It certainly was. The wind broke several windows. What weather!\n",
            "#Person1#: Do you know that big tree in front of my house? One of the biggest branches came down in the night.\n",
            "#Person2#: Really? Did it do any damage to your home?\n",
            "#Person1#: Thank goodness! It is far away from that.\n",
            "#Person2#: I really hate storms. It's about time we had some nice spring weather.\n",
            "#Person1#: It's April, you know. The flowers are beginning to blossom.\n",
            "#Person2#: Yes, that's true. But I still think the weather is terrible.\n",
            "#Person1#: I suppose we should not complain. We had a fine March after all.\n",
            "\n",
            "What was going on?\n",
            "#Person1# and #Person2# are talking about the heavy storm last night, and #Person1#'s positive. #Person2# thinks the weather is terrible. #Person1# thinks they should not complain.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "What was going on?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the prompt to perform the one shot inference:"
      ],
      "metadata": {
        "id": "blFL5EbQcUPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSwXziWrcZJd",
        "outputId": "74c16454-2d4e-4d35-f5b4-0edede1efa61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: #Person1# wants to upgrade his system. #Person2# wants to add a painting program to his software. #Person1# wants to add a CD-ROM drive.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3-Few Shot Inference"
      ],
      "metadata": {
        "id": "9x3qFM59elOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)"
      ],
      "metadata": {
        "id": "DrFNEOCXepyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the prompt to perform a few shot inference:"
      ],
      "metadata": {
        "id": "3ytQMiK7e0I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Generated Summary: {output}\\n')\n",
        "\n",
        "print(f'Human Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwjWjmiKe2fy",
        "outputId": "667741f3-69e0-4bf1-cac4-4c23cd2e451c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n",
            "\n",
            "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
          ]
        }
      ]
    }
  ]
}