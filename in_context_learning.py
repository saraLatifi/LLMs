# -*- coding: utf-8 -*-
"""in_context_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QE9nXl9ThocJxTsOEB1WLvBJe2ZO-2J

# Summarization

The first step towards prompt engineering and see how it can enhance the generative output of Large Language Models by comparing **zero shot**, **one shot**, and **few shot** inferences.
"""

!pip install datasets

"""Load the datasets, Large Language Model (LLM), tokenizer, and configurator."""

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer
from transformers import GenerationConfig

hf_dataset_name = "knkarthick/dialogsum"
dataset = load_dataset(hf_dataset_name)

dataset # 'summary' is human summary

model_name = 'google/flan-t5-base'
# AutoModel: load the model by its name
# ForSeq2Seq: for seq_to_seq task
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""**Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models.

`AutoTokenizer.from_pretrained()` method downloads the **tokenizer** for the **corresponding model**. Parameter `use_fast` switches on fast tokenizer.
"""

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

"""Test the tokenizer **encoding** and **decoding** a simple sentence.


The output of `tokenizer` is a **dictionary** containing tensors needed for a model: `input_ids` and `attention_mask`.

**input_ids**

- *Type*: **torch.Tensor** (because return_tensors='pt')
- *Contents*: A sequence of **token IDs** corresponding to your prompt.
- *Purpose*: These IDs are the **actual numerical representations** the model uses as input.

`prompt (text)  → tokenizer → input_ids (numbers) → model`
"""

sentence = "Learning Large Language Models (LLMs) is amazing!"

# return_tensors: define the type of the data structure
# pt: pyTorch (& tf: tensorflow)
sentence_encoded = tokenizer(sentence, return_tensors='pt')

print(f'ENCODED: {sentence_encoded}')

print(f'ENCODED: {sentence_encoded["input_ids"][0]}')

"""`skip_special_tokens=True` removes **special** tokens (e.g., **</s>** which is the *end of sentence* token).

"""

sentence_decoded = tokenizer.decode(
        sentence_encoded["input_ids"][0],
        skip_special_tokens=True
    )

print(f'DECODED: {sentence_decoded}')

"""# **0-Summarization without Prompt Engineering**

**Prompt engineering** is an act of a human changing the prompt to improve the response for a given task.
"""

# test on a dialogue (index=200) in the test set
dialogue = dataset['test'][200]['dialogue']
summary = dataset['test'][200]['summary']

inputs = tokenizer(dialogue, return_tensors='pt')

# try to get a summary JUST by limmiting the max_new_tokens
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        max_new_tokens=50
    )[0],
    skip_special_tokens=True
)

print(f'Dialogue: \n{dialogue}\n')

# the model was not asked for summarization => the output will no be a summary!
print(f'Generated Summary: {output}\n')

print(f'Human Summary: {summary}')

"""# **1-Summarization with an Instruction Prompt**

**Prompt engineering** is an important concept in using foundation models for text generation.

# 1.1-Zero Shot Inference with an Instruction Prompt

Wrap the dialogue in a descriptive instruction.

tokenizer.tgt_lang = "de"   # target language = German
tokenizer.src_lang = "en"   # source language = English

tokenizer.tgt_lang is typically an attribute used in tokenizers for multilingual machine translation models (like mBART, M2M100, NLLB, etc.) to specify the target language you want the model to generate.

It’s not a universal tokenizer attribute — it appears only in certain models where you must set a target language ID before generation.

# **1.1.1-Prompt 1**
"""

prompt = f"""
Summarize the following conversation.

{dialogue}

Summary:
    """

# Input constructed prompt (instead of the dialogue)
inputs = tokenizer(prompt, return_tensors='pt')
output = tokenizer.decode(
  model.generate(
      inputs["input_ids"],
      max_new_tokens=50,
  )[0],
  skip_special_tokens=True
)

print(f'Dialogue: {dialogue}\n')

print(f'Generated Summary: {output}\n')

print(f'Human Summary: {summary}')

"""# **1.1.2-Prompt 2**"""

prompt = f"""
Dialogue:

{dialogue}

What was going on?
    """

# Input constructed prompt (instead of the dialogue)
inputs = tokenizer(prompt, return_tensors='pt')
output = tokenizer.decode(
  model.generate(
      inputs["input_ids"],
      max_new_tokens=50,
  )[0],
  skip_special_tokens=True
)

print(f'Dialogue: {dialogue}\n')

print(f'Generated Summary: {output}\n')

print(f'Human Summary: {summary}')

"""# 1.2-One Shot Inference with an Instruction Prompt"""

def make_prompt(example_indices_full, example_index_to_summarize):
  prompt = ''

  for index in example_indices_full:
    dialogue = dataset['test'][index]['dialogue']
    summary = dataset['test'][index]['summary']

    # The stop sequence '{summary}\n\n\n' is important for FLAN-T5.
    # Other models may have their own preferred stop sequence.
    prompt += f"""
Dialogue:

{dialogue}

What was going on?
{summary}\n\n\n
"""

  dialogue = dataset['test'][example_index_to_summarize]['dialogue']

  prompt += f"""
Dialogue:

{dialogue}

What was going on?
"""

  return prompt

"""Construct the prompt to perform one shot inference:"""

example_indices_full = [400] # use as the example
example_index_to_summarize = 200 # use to test the model

one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)

print(one_shot_prompt)

"""Pass the prompt to perform the one shot inference:"""

summary = dataset['test'][example_index_to_summarize]['summary']

inputs = tokenizer(one_shot_prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        max_new_tokens=50,
    )[0],
    skip_special_tokens=True
)


print(f'Generated Summary: {output}\n')

print(f'Human Summary: {summary}')

"""# 1.3-Few Shot Inference"""

example_indices_full = [40, 80, 120]
example_index_to_summarize = 200

few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)

"""Pass the prompt to perform a few shot inference:"""

summary = dataset['test'][example_index_to_summarize]['summary']

inputs = tokenizer(few_shot_prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        max_new_tokens=50,
    )[0],
    skip_special_tokens=True
)

print(f'Generated Summary: {output}\n')

print(f'Human Summary: {summary}')