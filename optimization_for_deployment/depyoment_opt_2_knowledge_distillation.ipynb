{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki-XHQ-_NbZ5"
      },
      "source": [
        "Original Notebook: [7_1_knowledge_distillation_Llama.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/7_1_knowledge_distillation_Llama.ipynb) by [Pere Martra](https://www.linkedin.com/in/pere-martra/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt3JNHq1sFzX"
      },
      "source": [
        "**Teacher Model:** [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
        "\n",
        "**Student Model:** [oopere/Llama-3.2-1B-pruned-40pct](https://huggingface.co/oopere/Llama-3.2-1B-pruned-40pct)\n",
        "\n",
        "Vast AI Environment: GPU A100\n",
        "\n",
        "---\n",
        "Example in a production environment:\n",
        "- Teacher: a 10B-parameter LLM model\n",
        "- Student: a 1B-parameter LLM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyM9T6gevaJb"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxuMPefXTHHd"
      },
      "source": [
        "## Install Libraries & Configure Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wCwJEDytRSS",
        "outputId": "9ac5f3b0-a804-4401-bc8b-f229626d1617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pip install datasets==3.2.0 transformers==4.47.1 torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pip install torchvision --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cv_cwDsmXNJj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
        "from datasets import load_dataset\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0+cu128\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.23.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfwbpR3_SSfS"
      },
      "source": [
        "## Login to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPO7xiPDSRkw",
        "outputId": "f696b0cf-b9c6-4d3a-bd5e-5ac56faa34a1"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Hugging Face:  ········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "hf_token = getpass(\"Hugging Face: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysGAclkaFkc"
      },
      "source": [
        "## Download the Models\n",
        "The teacher model will be the same model used as the base to create the pruned model we are going to train.\n",
        "\n",
        "* **Teacher model:** `\"meta-llama/Llama-3.2-1B\"` (the base model)\n",
        "* **Student Model:** `\"oopere/pruned40-llama-3.2-1B\"` (the pruned version of the base model)\n",
        "\n",
        "In **some** scenarios the teacher model must be the **same** model used to create the pruned version.\n",
        "- Imagine you have a model that works perfectly and has been trained with proprietary **company data**, thus containing specific knowledge. In this case, if the goal is to replicate the behavior of this model in a smaller one, it wouldn’t make sense to use a larger model that hasn’t been trained on the same data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HTumwZbQXSgB"
      },
      "outputs": [],
      "source": [
        "# Load teacher and student models and their tokenizers\n",
        "teacher_model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "student_model_name = \"oopere/pruned40-llama-3.2-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "5950156ccb3d4a6296d351cfaa22770e",
            "8180402e5f0c4d57a16fd02d4c214ed7",
            "39aa590d9bca4059b69fd5d8d5e19b01"
          ]
        },
        "id": "W8Dn_sYAXVrH",
        "outputId": "6144d27c-b025-4ae8-d13e-d2956fb44574"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9b4e885cd4f49fcb1e91fbcea6038a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d30ae06d3fd649a5a4e29d481a854b1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "772ccbe2dffb48a1aeff58d747f3fd7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "# The same tokenizer can be used for both models since they're both Llama-based\n",
        "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1773d39ede7467482b6a7be3472f1f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bc613f7577944f4a3259113e51b12e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9951095a1d2444e7b7049d2ddb8c8f47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "f604590a0f3f4cc78f54fc14e9141db1",
            "81f24abfff6541eab62b73c2a41b5a1c",
            "62e6a981a5364063a3033f89ee312f8b",
            "0e9bc06d77174a34a88f33412683eab5",
            "6947a43599784837a566d76c703e51df",
            "ddb3b6f233844d62916dc6a7b6d09381",
            "7010428a6eb141bab26bbdd34adda664",
            "7e1e78abe71e458e9eef0a183668b84e",
            "971cbd5ce39e4cefbce876a75524f028",
            "894074dc01ed488fbc881b2d940bdc3d",
            "ffd739451c9f4344a7319779afeadbbd",
            "8a86aec8158842978d737c9cefa3124c",
            "f98a32c60315447bb7a5a45b3d10fa5f",
            "4e312c70a2a84a6aae2cfea08c25f6d3",
            "733294e8aa52423ab2f3cfa298a81977",
            "324b0397e8834d03ad580319e071c861",
            "6c4491087ef148bb851574a5b3027c18",
            "9e72bb13774546e7bb939cce1ed30d5c",
            "2314320a98574c54941b3b790ae7f411",
            "e0f1221d0f0042f88be949dfdbbf97af",
            "44e5bef3544943f3862879a6c86751ad",
            "e884965833f546dca840c30d5f6587ab",
            "b65a308614854ec9b94a28e635d828d2",
            "d3712cc08e4d4dcaa923b33898077e88",
            "1f1c24e735a443dab072c31c73d97dba",
            "7a1892098fb34d55b03dcee70356247b",
            "f16aff4b953d491c97b6c911356d3aac",
            "b686f72b941a4ea0a83c83f39fa020aa",
            "fef4011bae704ebf828a76184db669f2",
            "75ce0bc671684eefb048794db6b0b4ac",
            "8dff5c00dcd4482f97bfabb1c801ccec",
            "240b63e5a1324c13b19c1fd1aa8d789d",
            "1d0b26c25e7a4cceabb2d236e1251239",
            "96fe0ff74b9c470d9225d69c04c26b64",
            "beb64826d4e4492288a60d2ba0268d8f",
            "3d35089cfed14e2999b792691a83a5fd",
            "f9948f62e48c4382893139d08880b621",
            "3c18f21aa91e46749cce59039628ce6c",
            "d1d32efa66344ed3bcab0868d2838891",
            "d8e8f8c79e4f4186b3948e8f481eb020",
            "d75a05bc49bf4cf2943a3b4ab7fc712b",
            "e06fd408696149019175ccbc61c94bc6",
            "9e2adcfc01504fb28e1eb7c0d8bb73ed",
            "c006db9b846d4fa6acbabedbbaf81caf",
            "dd02823e3e1843d598a5ac7645997420",
            "e7f5ce14d2634eb09e991c2054bc2a43",
            "7f926efd39e24464b7aec070f065589d",
            "99f11932c1804a2d98cd8045dac59e43",
            "e784a3814f1e4694b2a103b99264a3f6",
            "881e3c47b98a4e13bc6d8c0caa7860ae",
            "e78927d6456b4d1db85f5e024ead0897",
            "4ecb26841fd840b8a9dde89c20e44182",
            "2cc0332edc7a4d2ebf75f9837bcac0a1",
            "97b9cb45f21d48a285ca7bfb729ad092",
            "04d9053100644f4cb41624b8e0e3f0ca",
            "4e96e0e00fa848c4be504ce5d450ce25",
            "7d4c39a4fde54073b8c0344a9e686144",
            "8509427d2f9843acb0cb2d1f180facc6",
            "e05e81bf046c4ca098b099b19c4093a0",
            "81fd5bc75785467cb67c8f477e46eea6",
            "8eb2b74d62664b969e439f0c94ae9d1f"
          ]
        },
        "id": "rYxtA9f0XWWv",
        "outputId": "bba60885-3cd4-498c-d2d7-dee3a18d63dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f91532dbfa88491180a1a95e661cc936",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d67fb3bec50c487c9178654853ae6c71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9409326a50db404abc185954ad74bf00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model\n",
        "student_model = AutoModelForCausalLM.from_pretrained(student_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWOsGcjzcPEx"
      },
      "source": [
        "# Load the Data\n",
        "\n",
        "\n",
        "During the pruning process, the model inevitably lost some capabilities, as expected.\n",
        "- In this case, the pruned model lost the generation capabilities much more tan the comprehension capabilities.\n",
        "\n",
        "The **dataset** to be used will largely **depend on** the results aimed to be achieved through the Knowledge Distillation process.\n",
        "- In this case, `Lambada` that showed the most degradation, both in its standard and OpenAI versions.\n",
        "  - This benchmark evaluates the model's ability to predict the last word of a text. However, these are not simple texts; the model must pay close attention since the last word needs to be inferred by considering the entire story, requiring understanding of broader context, coherence, and fluency.\n",
        "- Other suitable alternatives could include `BookCorpus`.\n",
        "- **Used benchmark:** a small portion of the `ptb_text_only` dataset because of both time and memory constraints.\n",
        "  - It may not be very suitable dataset, as it is (only) for text generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "7619910969af42578f641dbe2d07907c",
            "3c5e6c9f27f1449b9888a99d1cfeef64",
            "99d955c486ea4a1ba19c9da183c4a97b",
            "2c7db1d7b7ff414ea4916a8f98144fa3",
            "9c416ca9ea6549738689f8271ecc4e3b",
            "3b7a1f2e430f410ca976929ad4ab64d0",
            "70f49210734642c0a3b989880745725b",
            "79cdaa3ef0fc45a2b03e1b19978da189",
            "29ca2481e2664f6e9dfcc220eaaa2d7d",
            "507b0c1d9eb14355b6d57787940ec0c3",
            "ad7859fa150647b4909b15fce7cda503",
            "0ee42f0c5b0e49ff8c92295e11558e71",
            "edbef1208ed344408d41c6b3f55b6507",
            "0723c1dc9f6746febe5e2e3a3ecaec60",
            "8910af9fe65d4b2d96f877d07483c467",
            "70e63d8e094e478ea44548ac5cbdae4c",
            "4cc0b6258d8242ee96bda6e287b3aea7",
            "84f4fbc6575a44a192504def7f76b7d5",
            "56209091d57643929b3c8fd6b3308361",
            "b1e5e73d4004461cbc7b0b9fc25e4498",
            "5f954935181240ceb8eee0a10461b357",
            "00f6e1e6cfe14159b9ac3373901101b8",
            "79819174487c49199ccd7097d800ff3a",
            "c320b103e04943f9b72ebf433552b0ab",
            "9714e13de754423f938da3bb85c45b18",
            "fef51d26a40d42818a2f462791dac7d6",
            "dafe3c2b12aa4d1fa13ba6144c5e448b",
            "7c8533cb75a1497891ed09340e200f59",
            "1ce1fc03c7f3412c94469a5b71766b9e",
            "e4162aab39f44471bc5e8810f6eb0076",
            "590d1d7da6e046f597d8169a5686ceab",
            "14224286c768489887d1ffc30a450d12",
            "0db6699821ca4918bd6d948d19489783",
            "5d4f971f8a534642aea0ebfb646c36e2",
            "9b6dde42068a44a1b7532eba5151deb7",
            "3f49ebee74d2491098f9587971cae211",
            "6e0c0c3792d34ec8b3322167770a9476",
            "358d29f2550b4c15811a753367a67a56",
            "99a0d130e0fd41d5a5b8c294961fe56e",
            "1ad1921fbfaf4b00966e143b0e731d45",
            "74bfa72a0ed540039585e962a56490b1",
            "0faf81b02622419d8da635c61b0b8349",
            "784726d604ab4d98987beffb9334af42",
            "7aa6f669da76495c83ce512c514eb5dd",
            "6286fabf9d5146ab913ce19884bbcc95",
            "4b0055301de144648a5923440a5278c7",
            "e333fcb696ef4a11af9960d1ecb5663a",
            "4e9b3250b66a40698b42a1ff9c1ca9fe",
            "92231be3ccf44749a4ba0282ea8b60ff",
            "305f0efe696442999ee91cce7616e2c4",
            "dac6ea90e47240f0a467ca66c798c648",
            "98a9b2ab24414f2286d083ba5e8cf0d1",
            "4ca4f9340b00462c9a3178c9a989b682",
            "cfc88ce6d1d444d4b31f7e4d0a88b410",
            "28c53e8916634e21aeed285e594c7992",
            "ccc4a1b6b2c340c0856d3d52fa5d7892",
            "9c7731738d0c4173b6b9f435886ba6c2",
            "d5c03c63dc6046ed8e1d7c78bf09b8f5",
            "2b968caab8704a399739794e73c190d9",
            "466229336bdd4317a6b667bcaa5a0954",
            "59f0662e471944e6b9fd513eb9055417",
            "55e389f904044208b13be4645e3455e6",
            "a21fb0643d4b4b8389ccfb1aec009992",
            "c75811e9e5ad4a3fa43cb7ef7aa3d212",
            "6c3e4cbc08ff4306960983f21fc6cbfc",
            "c4a5ac430ca9455a8c18ee7b3700e032",
            "f30e7ff41eed4d498df5bb24944436ca",
            "aa961d9cde564631994db8641c47f373",
            "af80dfaec3b94e6a9f378ddc1d9f8045",
            "860fbffd99494a9694e45f4ad98b3c56",
            "dbb29a07c9814ae7a6aa415b1028b591",
            "fd476726cc0a4d4193d05f9bf17e8cb8",
            "e7c9459c34bf47859d7b6c5f83fcc798",
            "97d7921ea7424cf4bd81168a887682ee",
            "6a4bd9a19dae487aa5669c4831afdbc4",
            "505466e15c2c44b99836cca04955cba0",
            "9d640a73fa074866b2d77080e2a203d3",
            "8a3b393b58204d50beecc9cda07e9ca2",
            "fd54d33f5a7944f188cf9f6fa335797f",
            "cbc23301b59543afbf1ae000039c62e5",
            "f67c0f5404cd42aa987606e8a3031443",
            "747786a0028d467c8cbfccf4c8e6b2b5",
            "61c14535cc4b488ab9dd0b46a50f695c",
            "f36fc27dd34a41d58dc0c37688c78022",
            "d8f5cfe0389e4b1789d963d0c06a88d9",
            "c28c99939e1947fdbcf34481ef9cb758",
            "6cb1dc3372bc4081a8bcff3ce5d0c071",
            "9f272c42525e4631923e31222fcf2083",
            "0780af4507634b858747b507f7642365",
            "eff2df681eaa48a4a36cbd8869bc7247",
            "554b4f321f944a028c4e06322874a70f",
            "c0207b0da9c042c894dfb9e2681942d2",
            "5dd6e0c0cd3e4a9e819fc6991ede2dbc",
            "9888a4c6f5fb4c4fa8bc7fbb3693b65a",
            "4001bb21fcff48fa8cbd51f4452c6908",
            "ec316800448542a283f355d327667222"
          ]
        },
        "id": "eVrAY9lkXYVI",
        "outputId": "704c45fd-92dd-4af4-c90a-efecde36e931"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf61f34e00de4aeba360993e4ded59be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db4c3427db3f4cae9257b198ed50eaca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ptb_text_only.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b19fbb4703f4d0ca2a60f95869cfa95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/5.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c3af28fbf4a44609a03e41b3bd693bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/400k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "608dee23b4d549f3a2a7f9c4aae58656",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/450k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "585ead29a5594a20b076224e7c01488a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/42068 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdff130718af413f814ea5b407d087cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/3761 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4cab468339842f1a110620977002262",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3370 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data Loading\n",
        "dataset = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"train\", trust_remote_code=True)\n",
        "# Take a subset for faster training/testing\n",
        "original_dataset = dataset\n",
        "dataset = dataset.select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1LpSiF2k1Iz"
      },
      "source": [
        "The `tokenize_function` is where the real preprocessing magic happens: it transforms raw text into a format that the models can understand.\n",
        "\n",
        "The function expects a dictionary input with a key `sentence` that contains the text to be tokenized.\n",
        "The text is procesed using the tokenizer previously loaded. Using the parameters:\n",
        "- `padding`=\"max_length\": ensures all sequences have the **same length** by adding padding tokens\n",
        "- `truncation`=True: cuts off sequences that are too long\n",
        "- `max_length`=128: sets the maximum sequence length, suitable for Llama models\n",
        "- `return_tensors`=\"pt\": returns **PyTorch tensors** instead of lists\n",
        "\n",
        "Then, the function prepares output:\n",
        "- Creates **input_ids**: the numerical representation of input tokens\n",
        "- Creates **labels**: in this case, identical to input_ids (clone) for language modeling\n",
        "- Returns **attention masks** to indicate which tokens are **padding** vs. **real content**\n",
        "\n",
        "In **knowledge distillation**, the aim is to transfer knowledge from a larger teacher model to a smaller student model. The **quality** of this process heavily depends on **how data is prepared**.\n",
        "\n",
        "- The careful **padding** and **truncation** ensure that all sequences are properly formatted for both teacher and student models.\n",
        "- The **attention masks** help models focus on relevant parts of the input\n",
        "- The **consistent sequence length** (128 tokens) optimizes memory usage while maintaining enough context for learning\n",
        "\n",
        "**Note:** We are setting up for language modeling specifically, which is why the labels are identical to the inputs. In language modeling, the task is to predict the next token given the previous tokens, so each input sequence serves as its own target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fIoi2PiwXax1"
      },
      "outputs": [],
      "source": [
        "# Create a tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize with padding and truncation\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"sentence\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,  # Adjusted for Llama models\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Create input_ids and labels for language modeling\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    labels = input_ids.clone() # create a copy of input ids\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04u_svapBJc"
      },
      "source": [
        "Time to use the `tokenize_function` to tokenize the Dataset. This code uses the datasets `map` function, which is specially **designed for processing large datasets**.\n",
        "\n",
        "Parameters:\n",
        "- `batched`=True: Processes multiple examples in batches for efficiency.\n",
        "- `batch_size`=32: Specifies the size of each batch during mapping. A **smaller batch size** ensures compatibility with **memory constraints**.\n",
        "- `remove_columns`=dataset.column_names: Removes original columns after tokenization to **avoid redundancy** and reduce memory usage.\n",
        "- `num_proc`=4: Enables **parallel processing** with four processes, speeding up the operation on large datasets.\n",
        "- `desc`=\"Processing examples\": Displays a description in the progress bar for better clarity.\n",
        "- `load_from_cache_file`=False: Disables caching to **ensure fresh processing** of the dataset, which is helpful during debugging.\n",
        "\n",
        "The `tokenized_datasets` object contains the preprocessed data with *input_ids*, *attention_mask*, and *labels* - ready for use in model training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "6a203eb168064ff491f40964081cc896",
            "f9f3b69c717443cbaa68689067a1a614",
            "9dc0aec9188c4ea4bc1e40c14c7076ee",
            "75e42bbc061149d385b3f3f3521c1773",
            "d004caa10a4746c9951a88abd4bb70db",
            "fbfb2fed55ce4ff088b4c4eea3851a74",
            "a9a4714bf6384a9bae53ffaa7b1daccf",
            "4ae453412b7045b0866d44754f445b05",
            "6e5ce4a07bf04549901fbc38754da217",
            "2c2bae1897b14bf39ff8cf7b6ead6b30",
            "90008225a1e44235b41b6132a54a981f",
            "f9619e94c91642a9b6aeda9259e5ae5d"
          ]
        },
        "id": "LE4-x7n6XeBh",
        "outputId": "87065478-747e-4125-c3b5-dd4bdc4059a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38c774152fed4d7fbd366844ad5c47a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing examples:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Process the dataset with progress bar\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,  # Smaller batch size for mapping\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Processing examples\",\n",
        "    load_from_cache_file=False  # Disable caching for debugging\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmPd8oTChLc"
      },
      "source": [
        "The format has to be converted to `torch`, making it compatible with PyTorch functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1h4jqaVpXggS"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch format\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJpHmkP3ucau"
      },
      "source": [
        "Set up how data will be fed into the models during training:\n",
        "\n",
        "The `DataLoader` is a PyTorch utility that **efficiently handles batching and iteration** over the dataset.\n",
        "- `batch_size=`: Specifies the number of samples per batch.\n",
        "  - A smaller batch size is used here due to the memory constraints of large models like Llama.\n",
        "- `shuffle=True`: Randomizes the order of data samples in each epoch.\n",
        "  - It improves the model’s **generalization** by reducing the likelihood of learning spurious patterns from data order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I00eLuHCXiAw"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "dataloader = DataLoader(\n",
        "    tokenized_datasets,\n",
        "    batch_size=8,  # Reduced batch size due to model size\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpiYF-tPvBB0"
      },
      "source": [
        "# Knowledge Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ6eeIMawBAf"
      },
      "source": [
        "Start moving the models to a cuda device (GPU), if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1LelQCbXku-",
        "outputId": "b0a4eaba-9294-48ad-d54c-d2cd015701f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=4916, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=4916, bias=False)\n",
              "          (down_proj): Linear(in_features=4916, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher_model.to(device)\n",
        "student_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNrAZFM3F0pr"
      },
      "source": [
        "## Teacher: Evaluation Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTe__XtOwfQC"
      },
      "source": [
        "**Crucial for knowledge distillation:**\n",
        "\n",
        "\n",
        "Put the **teacher model** into **evaluation mode**, which:\n",
        "- Disables dropout layers\n",
        "- Freezes batch normalization statistics\n",
        "- Ensures consistent outputs for the same inputs\n",
        "\n",
        "**Why Important:**\n",
        "- The teacher model should provide **stable, consistent predictions** to guide the student\n",
        "- The teacher model is **not trianed** anymore - it's only being used to generate \"soft targets\"\n",
        "- Any **randomness** (like dropout) would make the **knowledge transfer less reliable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIpYWhCVXmwd",
        "outputId": "1c9d641b-8a39-4eee-c2f0-9277c2bb6884"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set teacher model to evaluation mode\n",
        "teacher_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadbVTOgOk_9"
      },
      "source": [
        "## Student: Trainig Mode\n",
        "\n",
        "The training process in knowledge distillation involves transferring knowledge from a larger teacher model to a smaller student model, with the idea that the student model mimics the behaviour of the teacher model.\n",
        "\n",
        "### Optimizer & Training Loop\n",
        "\n",
        "The optimizer updates the student model's parameters to minimize the loss, improving its ability to replicate the teacher's outputs.\n",
        "- **AdamW** is kind of a **standard** for **trasformers** based models.\n",
        "\n",
        "Hyperparameters' Role:\n",
        "- `temperature`: Controls how \"soft\" the teacher's predictions are made.\n",
        "  - **Higher** temperature (2.0) **smooths out** the probability distributions.\n",
        "- `alpha`: **Balances** the importance of **matching the teacher's predictions** versus **ground truth**.\n",
        "- `accumulation_steps`: Allows for **larger effective batch** sizes without increasing memory usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "L8qw4oqQXo6d"
      },
      "outputs": [],
      "source": [
        "# Define optimizer for student model\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-5)  # Reduced learning rate for Llama\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "temperature = 2.0  # Increased temperature for Llama\n",
        "alpha = 1  # Weight for soft loss\n",
        "\n",
        "accumulation_steps = 4  # Gradient accumulation for larger effective batch size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoXC16w5JUxG"
      },
      "source": [
        "**Note:**\n",
        "In this case we do **pure knowledge distillation**:\n",
        "- We are **not** training the model **from scratch**, but **only** training it **to mimic** the teacher’s behavior.\n",
        "  - `alpha` parameters is **not used**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "9GGt0sbdXr5j",
        "outputId": "e60f6af3-9fdf-4e4b-aadc-a725ef2409d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Average Loss: 30.1915\n",
            "Epoch 2/10, Average Loss: 8.3191\n",
            "Epoch 3/10, Average Loss: 6.3736\n",
            "Epoch 4/10, Average Loss: 5.4338\n",
            "Epoch 5/10, Average Loss: 4.8372\n",
            "Epoch 6/10, Average Loss: 4.4554\n",
            "Epoch 7/10, Average Loss: 4.1792\n",
            "Epoch 8/10, Average Loss: 3.9438\n",
            "Epoch 9/10, Average Loss: 3.7438\n",
            "Epoch 10/10, Average Loss: 3.5864\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ### 1 - Model Preparation ###\n",
        "    # initializes each training epoch,\n",
        "    # putting the student model in training mode\n",
        "    student_model.train()\n",
        "    # Initializes total_loss to track the cumulative loss for the epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        ### 2 - Data procesing.  ###\n",
        "        # Moves the batch data to the appropriate device (CPU/GPU) for processing.\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        ### 3 - Teacher Model Inference ###\n",
        "        # Disables gradient computation to save memory and speed up inference.\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            # Applies temperature scaling to soften the teacher's predictions\n",
        "            teacher_logits = teacher_outputs.logits / temperature\n",
        "\n",
        "        ### 4 - Student Model Inference. ###\n",
        "        # The student model generates logits for the same input data.\n",
        "        student_outputs = student_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        student_logits = student_outputs.logits\n",
        "\n",
        "        ### 5 - Compute loss ###\n",
        "        # Converts logits to probabilities using softmax\n",
        "        teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
        "        # Computes the KL Divergence between the teacher's probabilities and the student's log probabilities.\n",
        "        # KL Divergence measures how well the student's predictions match the teacher's.\n",
        "        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "        # Note: for more stability, \"kl_div\" takes 'student_log_probs' but not 'student_probs'\n",
        "        loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
        "        # The loss is divided by accumulation_steps to balance gradient updates across accumulated batches.\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        ### 6- Backward pass ###\n",
        "        loss.backward()\n",
        "\n",
        "        ### 7 - Optimization Gradient Accumulation ###\n",
        "        # Accumulates gradients over multiple batches\n",
        "        # Updates model parameters when enough gradients are accumulated\n",
        "        # Resets gradients after update\n",
        "        if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(dataloader)):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        ### 8 - Loss Tracking ###\n",
        "        # Scales the loss back up by multiplying it with accumulation_steps to reflect the actual batch contribution.\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # if (batch_idx + 1) % 100 == 0:\n",
        "        #    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    ### 9 - Epoch-Level Reporting\n",
        "    # Computes the average loss for the epoch by dividing the total loss by the number of batches.\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c723Pp62D3JG"
      },
      "source": [
        "# Store the Model\n",
        "At the end of the training Loop, we have a model that can be store or uploaded to Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w1Zt2utRO7HK"
      },
      "outputs": [],
      "source": [
        "student_model_name = \"pruned_distilgpt2_kd_gem\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eIwVF1NyXvCv",
        "outputId": "3803bb9b-9d70-4206-99ea-6fbdd5ab8d1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('pruned_distilgpt2_kd_gem/tokenizer_config.json',\n",
              " 'pruned_distilgpt2_kd_gem/special_tokens_map.json',\n",
              " 'pruned_distilgpt2_kd_gem/tokenizer.json')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the fine-tuned student model\n",
        "student_model.save_pretrained(student_model_name)\n",
        "tokenizer.save_pretrained(student_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a08807d980114b8981b3942469b7ebb1"
          ]
        },
        "id": "-eVy_BYRO-P2",
        "outputId": "b1bc166a-fd9a-4aff-fd29-9a3cc912b322"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5272fb943f44f9b80c70fdb405b1c05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4dbd1b63a714fb5992817dcba4211d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce920de8ca2141eb897fca36794d2388",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...distilgpt2_kd_gem/model.safetensors:   0%|          |  553kB / 3.66GB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Saralatifi/pruned_distilgpt2_kd_gem/commit/2272eb9de53b4addafddd337a2c6eb6750539069', commit_message='Upload LlamaForCausalLM', commit_description='', oid='2272eb9de53b4addafddd337a2c6eb6750539069', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Saralatifi/pruned_distilgpt2_kd_gem', endpoint='https://huggingface.co', repo_type='model', repo_id='Saralatifi/pruned_distilgpt2_kd_gem'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student_model.push_to_hub(student_model_name,\n",
        "                  private=False,\n",
        "                  use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0d3cc42bc25a4afba2ae26238fb16715",
            "6b5505e09e974554b2fccbe0bade2dad"
          ]
        },
        "id": "I6EEOIVkPAVQ",
        "outputId": "4006c6bf-8f5c-44b5-9ebc-e3b895a2bfed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0f46f3b5fde479aa016b84e76e98b68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b1f0978fa8c4488a2be53810d8493fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "478e42124a664ff0b4b9a219fdd77b40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af0a9947f9a24dd6a31e1fe3fcc72b2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...ed_distilgpt2_kd_gem/tokenizer.json: 100%|##########| 17.2MB / 17.2MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Saralatifi/pruned_distilgpt2_kd_gem/commit/e9a053a26acfab8d1cbe46cdcac04a3e4746f320', commit_message='Upload tokenizer', commit_description='', oid='e9a053a26acfab8d1cbe46cdcac04a3e4746f320', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Saralatifi/pruned_distilgpt2_kd_gem', endpoint='https://huggingface.co', repo_type='model', repo_id='Saralatifi/pruned_distilgpt2_kd_gem'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.push_to_hub(student_model_name,\n",
        "                      private=False,\n",
        "                      use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC2Jy9B4Er8P"
      },
      "source": [
        "# When to use KD versus other forms of fine-tuning?\n",
        "There are different efficient ways to introduce knowledge into a model: LoRA and QLoRA. Their use compared to **KD** serves **different purposes**.\n",
        "\n",
        "**KD** helps us imitate a model:  we might have a model that has already been fine-tuned with our data and gone through a Pruning process. To **recover the lost capacity**, the best approach is to perform a KD process from the base model.\n",
        "- In general, we could use **LoRA** or **QLoRA** to improve the model's performance, and we would benefit from the reduction in trainable weights that these two techniques bring."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}