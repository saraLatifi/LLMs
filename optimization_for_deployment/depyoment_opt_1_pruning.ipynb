{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployment Optimization: Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT0FAsUnncFY"
      },
      "source": [
        "Original Notebook: [6_3_pruning_structured_llama3.2-1b_OK.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) by [Pere Martra](https://www.linkedin.com/in/pere-martra/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noxn_NEM7bUB"
      },
      "source": [
        "**Model:** meta-llama/Llama-3.2-1B\n",
        "\n",
        " - Request for the model:\n",
        "    https://huggingface.co/meta-llama/Llama-3.2-1B\n",
        "\n",
        " - check request for permission:\n",
        "    https://huggingface.co/settings/gated-repos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQIxAOPZtPBN"
      },
      "source": [
        "# Install libraries & Configure variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** `sys.executable` ensures that pip installs into the same Python environment that Jupyter is using.\n",
        "\n",
        "`sentencepiece` is required for LLaMA tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zHApVm41HWq",
        "outputId": "6f802174-3a90-4211-f121-01a864b3df29"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets transformers torch sentencepiece tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GJNgRj4M187E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbIyUlXEtbqs",
        "outputId": "81f51518-7e22-4b22-edab-b436851ef59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYtphuD4tPIt"
      },
      "source": [
        "## Log in to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVDn0lc_tMRY",
        "outputId": "c2bfd296-72e7-4cc6-803a-78023722bac9"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Hugging Face:  ········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "hf_token = getpass(\"Hugging Face: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMghcumAtXZA",
        "outputId": "7b1ebf08-31ab-4811-8419-4a925e76ce9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "The token `llama_token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llama_token`\n"
          ]
        }
      ],
      "source": [
        "!hf auth login --token $hf_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM-QwxyKw-YG"
      },
      "source": [
        "# Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "581d403800db40cb94f1f852ed8e4745",
            "efe762ed3e444fa59131eeaa02b0cf31",
            "0c6e531a68284d7a9c63c1c84b3cfe31",
            "4b2034e87deb4dd38ff1e536238179e5",
            "7d3a4273616e4770908b5cf292a627c7",
            "e970077942dd430fbce5cd6364168b96",
            "a52913aef34f405a948cd23a9e7b6edf",
            "7ad3e3cc410a493493e5094160c9f942",
            "7687504cd4cf4bd8a871afe13df152b6",
            "e70c59a8d78d40bcb0bd984b421b0e03",
            "bdb500c9137248b9a3cebda17e5e0059",
            "fc701feafc7542b3a631b5b68ad87e1a",
            "f69e8919ea0646f19e62b1a1641d9971",
            "b6941f1febac4a4b81388548cfccfdf7",
            "1f1dab7810bb4ee2872623424f9e7ec7",
            "0469083297bd4cf287976e47fcbe6bfa",
            "9a749e8c64b84825ba1a4ae1642a9604",
            "923008944dfe443db1bf5525f198a406",
            "98c76fa242a541ae97c3416dc98db7b2",
            "446445a3873e4d99ab86bd43e157b1ae",
            "90f4c435279a49519969900bbb2db933",
            "b20be8db400e4f54973486e77749e423",
            "fb1924b3c9994a668551c1a1ff9ae73d",
            "ca3f9362006942c1853f7b6aba2a82f3",
            "2cdd673ed80e4fb7b15f8f4e5ef3ffb2",
            "b7eeafe7ec5b4080a0a208dc14e94fe9",
            "6c4ee9e2ce9a410bbbc93454c42dc4db",
            "64d892939bec4b3da29ceccefc1a630c",
            "8fe110cd0cf84100b569d7ae8c54111b",
            "90a87111a8724481919abbddda633f9d",
            "f0d7ff58109442f6b4b7743d2888c501",
            "fe3c994a4076420fa31a094fe0183b0d",
            "140d64387dd94659b7a9644f320be1eb",
            "c9691db47dc94fabaa067e7ff5b50303",
            "9422b00932ff4718ba7f5b350bff6c03",
            "b133e6fcc00d4db9a1edad9836eb0a30",
            "647cb791c21642cc9137e97f597e3663",
            "23871ed43c3844f68f9a34f90cf51b51",
            "1715c067d6a7434297abf78c04ca6f67",
            "20f3fbd65223462483e1345d62bb501b",
            "07dfb9d672694788b78bb9455ff7789e",
            "788ff57882af458c93dc458a9665a402",
            "ad32149064b949c988aed8ae5e11d49b",
            "e4a20eca03584c29aa4e130be9f46e8a",
            "3fa1e69962e344bb814c8eb98038bfe4",
            "8afe5821dc3b484f80143ab80479eade",
            "8c04c23f2f0e4febae4195e79efdd8df",
            "5d0dd428cbac411186ef7bccf6f9863f",
            "104e344666d947248a6bda0185d20c1a",
            "bcf87d49f2754263b567f090db7a222f",
            "a79688c3711945a6a725aa9dfb9f650e",
            "9ce86bb2196b4ba7a8c236ea8ced47cb",
            "70e649c460104d369a3f0981c5b256b3",
            "3dd1d55fd09e4fd893c306e93b8305a3",
            "71fc0fc0ce9f44a8b5c0eeef8e5e78ee",
            "948849bb11af45baa0ebe60e2eb88fac",
            "c60b1d24fee041e3bf58c6f36b1ddaf8",
            "36ca32eae0af40c1a7e36210123ad604",
            "243eaa84eba846969ab718ace7018c8f",
            "4a6e07e789d14818b463c96f1da54017",
            "fcf349420473489d87bb35dff6c904f6",
            "8cc943ce39004af9b3144a665e8f39e4",
            "dd7c0381478f49c5a5d189e68d50b139",
            "a056851424db4a3abdfcc589936687e2",
            "8a8c063f4ac543728e6f5e2b107677ef",
            "5a245e0640544a148ca53634814d315c"
          ]
        },
        "id": "q-z_1Zpg2I6u",
        "outputId": "c4799450-b6c6-4b12-807a-2e937d873e91"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df598ec527b44a9b1c83f1f50b4b05f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a733fe90b4d7499ea61e03f7d2b9d9df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d4c3d71e11b4a118e0fba1a5940ee88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "030482b89c674c3fa81195d30387b710",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d26493dfb7004f59821fe1aa6d893836",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a4898f51cea491ca198810e655a5527",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set pad_token_id to eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9UpMD4Hw2MWg"
      },
      "outputs": [],
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'], # to know which parts of the input are important and which parts are padding\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4muyx_8M5OAu"
      },
      "source": [
        "## Study the Model Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** to prune a model, it is necessary to understand the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Hs4oQ4B7Z0",
        "outputId": "74329c97-9453-4374-9f48-eac50dd8ce91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMslK3QCAb1"
      },
      "source": [
        "**Pruning Llama**: based on the observations pruning did not help on **attention** or **output** blocks. The best layers was **MLP** block.\n",
        "\n",
        "An **MLP block** typically consists of layers that scale the data to larger dimensions and others that return it to its original size.\n",
        "- It gives more capacity to the model (to solve more complex problems).\n",
        "\n",
        "In the MLP block of the model, there are two projection layers: `gat_proj` and `down_proj`, both scaling from 2048 to 8192. The purpose of having two layers projecting to the same intermediate size might be related to gating mechanisms. A gating mechanism selectively controls information flow in neural networks by using learned weights to \"gate\" or filter inputs.\n",
        "- However, to truly understand how these layers function, it is necessary to refer to the model's documentation or even its source code. Nevertheless, this structure usually indicates that the layers performing the upsizing work in pairs and cannot be treated as independent linear layers.\n",
        "    - In other words, any operation applied to one layer must be replicated in the other. Most importantly, when identifying which neurons are more or less important, do not evaluate the neurons of a single layer in isolation; instead, treat them as pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alKH3QH64WFL",
        "outputId": "152e3c9e-bb1f-4230-e212-80717719b66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: Vienna is the capital of Austria and one of the most beautiful cities in the world. The city is located on the Danube River and has a population of over 1.8 million people. Vienna is known for its rich history, beautiful\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Vienna is the capital of\"\n",
        "generated = get_output(prompt)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8WR96iwq2XYH"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kph43oObnet7",
        "outputId": "343d8bf0-fd2b-489b-bd52-d040ee69a9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model parameters: 1235814400\n"
          ]
        }
      ],
      "source": [
        "original_param_count = count_parameters(model)\n",
        "print(f\"Original model parameters: {original_param_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9NwmBWnkSP"
      },
      "source": [
        "# Pruning\n",
        "## Support Pruning Functions\n",
        "### Compute Neuron Importance Functions\n",
        "\n",
        "Three functions are used to calculate neuron importance, in order to decide which ones to eliminate.\n",
        "- All three functions take into account that the layers should be treated as pairs, considering both layers to calculate neuron importance.\n",
        "\n",
        "The results obtained with each function have been quite different:\n",
        "- **Product of Norms**\n",
        "- **Variance of weights**\n",
        "- **Maximum absolute weight**\n",
        "\n",
        "**Observation:** the **Absolute Maximum** calculation has worked the best. I'd say the other methods for selecting neurons to remove have severely degraded the model, or at least eliminated a significant portion of the base model's capabilities.\n",
        "\n",
        "*I’m leaving the others in the notebook purely as an exercise.*\n",
        "\n",
        "The **Maximum Absolute Weight** method works better because it directly identifies the most influential neurons based on the magnitude of their connections. These neurons are likely responsible for key decisions, making the model more accurate after pruning.\n",
        "- The **Variance of Weights** method, while useful in some contexts, can retain neurons that may not contribute significantly to the task, leading to less coherent model outputs.\n",
        "\n",
        "**Warning:** do not fall into the trap of assuming that this neuron selection method will work best across all model structures. It works well with *Llama* models, and this may be due to several factors:\n",
        "- The relatively large projection from 2048 to 8192.\n",
        "- The use of a GLU structure.\n",
        "- The type of activation function used.\n",
        "\n",
        "**Note:** when using a model from another family (e.g., Gemma or Mistral), the neuron-selection method might need to be entirely different.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fflgE2Y8_eZF"
      },
      "outputs": [],
      "source": [
        "#****DISCARTED****\n",
        "\"\"\"\n",
        "Product of Norms\n",
        "\n",
        "Note: since the GLU multiplies the outputs of gate_proj and up_proj,\n",
        "compute the product of their weight norms to better represent the importance of the neuron pair.\n",
        "\n",
        "Sample output:\n",
        "'Paris is the capital of of of of the of the the the the to to to from to from \n",
        "from from to to from to France France France France France France France France\n",
        "France France France France France France France France All All All'\n",
        "\"\"\"\n",
        "\n",
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "\n",
        "    gate_norms = torch.norm(gate_weight, p=1, dim=1)\n",
        "    up_norms = torch.norm(up_weight, p=1, dim=1)\n",
        "    importance_scores = gate_norms * up_norms\n",
        "    return importance_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_mMGhTtD4PRX"
      },
      "outputs": [],
      "source": [
        "#****DISCARTED****\n",
        "\"\"\"\n",
        "Variance of Weights\n",
        "\n",
        "Note: neurons with higher weight variance may contribute more to the model's output.\n",
        "\n",
        "Sample output:\n",
        "'Paris is the capital of the French Republic. It is also a Paris is the\n",
        "capital of the French Republic. It is also a Germany is the German Republic.\n",
        "It is also a of the Austrian Republic. It is also a'\n",
        "\"\"\"\n",
        "\n",
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "    gate_variance = torch.var(gate_weight, dim=1)\n",
        "    up_variance = torch.var(up_weight, dim=1)\n",
        "    importance_scores = gate_variance + up_variance\n",
        "    return importance_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Seyqaquj7mQA"
      },
      "outputs": [],
      "source": [
        "#****SELECTED****\n",
        "\"\"\"\n",
        "Maximum Absolute Weight\n",
        "\n",
        "Note: the maximum absolute weight in a neuron might indicate its significance.\n",
        "\n",
        "Sample output:\n",
        "'Paris is the capital of France. It is also one of the most beautiful cities in\n",
        "the world. There is so much to see and do in Paris that it is impossible to cover\n",
        "it all in one day. However, there are a few things you should not miss while you'\n",
        "\"\"\"\n",
        "\n",
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "  \"\"\"\n",
        "  compute neuron pair importance scores (Maximum Absolute Weight)\n",
        "\n",
        "  Args:\n",
        "  - gate_weight: Weight matrix from the gate_proj layer.\n",
        "  - up_weight: Weight matrix from the up_weight layer.\n",
        "\n",
        "  Returns:\n",
        "  - importance_scores: Importance scores for each neuron pair.\n",
        "  \"\"\"\n",
        "\n",
        "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "  importance_scores = gate_max_abs + up_max_abs\n",
        "  return importance_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NX9Boph94RWA"
      },
      "outputs": [],
      "source": [
        "# Prunes a specific percentatge of neurons from the MLP (feed forward layers)\n",
        "def prune_neuron_pairs(mlp, prune_percent):\n",
        "    \"\"\"\n",
        "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
        "    layers removing the least important neurons.\n",
        "\n",
        "    Args:\n",
        "    - mlp: Layers to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
        "    - k: New intermediate size.\n",
        "\n",
        "    \"\"\"\n",
        "    # Extract the weights from the MLP layers\n",
        "    # these weights are used to calculate each neuron's importance score in the next step.\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    # Compute importance scores\n",
        "    # Neurons with higher importance scores are considered more important and less likely to be pruned.\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    # Store the original number of neurons in the intermediate layer.\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    # Computes the number of neurons to prune.\n",
        "    # num_neuron_pairs_to_prune = prune_percent * original_intermediate_size\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    # Calculate the number of neurons to keep: the new intermediate size.\n",
        "    keep = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    # Check that there is no big error calculating keep, i.e., do not prune all the neurons!\n",
        "    if keep <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {keep}. Adjust the prune_percent.\")\n",
        "\n",
        "    # Select the neuros to keep, by obtaining the indices to keep.\n",
        "    # _ & indices_to_keep <=> the top highest values & the indices of these top values\n",
        "    _, indices_to_keep = torch.topk(importance_scores, keep, largest=True, sorted=True)\n",
        "    # indices of the top most important neurons sorted by importance (not by their original order)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    # Create the new layers\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, keep, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, keep, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(keep, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    # Copy weights to the new layers\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    # Return new layers and intermediate size\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, keep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0v_RpeST87"
      },
      "source": [
        "# Prune Loop\n",
        "\n",
        "The `update_model` function iterates through the blocks *(16 blocks)* within the model's Transformer structure. This structure consists of multiple `LlamaDecoderLayer` blocks, and each of these blocks contains a pair of `LlamaSdpaAttention` and `LlamaMLP` components. The latter contains the MLP layers that will be the target of the pruning process.\n",
        "\n",
        "```\n",
        "(layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "  )    \n",
        "```\n",
        "\n",
        "The layers that will undergo the removal of neurons identified as less useful are:\n",
        "\n",
        "```\n",
        "(gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "(up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "(down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "```\n",
        "\n",
        "The neurons are removed in the `prune_neurons` function based on the values returned by `compute_neuron_pair_importance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FxJEWg1X3j0m"
      },
      "outputs": [],
      "source": [
        "# Iterates throught the model layers and applies pruning.\n",
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    It modifies each mlp layer present in model, to retain only the most\n",
        "    important neurons. Creating new smaller versions of each layer pruned.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model.\n",
        "    \"\"\"\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    # loop for each model layer.\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        # each layer is a LlamaDecoderLayer it contains multiple components: Attention, MLP and Layer norms\n",
        "        # targetting MLP component by accesing layer.mlp.\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        # Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
        "\n",
        "        # Replace the Origiginal Layers with Pruned Layers.\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        # new_intermediate_size only needs to be set once\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    # Update the model config file.\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHtSbRmS267"
      },
      "source": [
        "# Prune the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NIUnFU5R3n42"
      },
      "outputs": [],
      "source": [
        "prune_percent = 0.2  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdJUkfWI3qMM",
        "outputId": "4712ab9f-0faf-464f-871d-1a2463e90763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model parameters: 1074792448\n",
            "Reduction in parameters: 161021952\n",
            "Percentage of weight savings: 13.03%\n"
          ]
        }
      ],
      "source": [
        "# Recalculate the number of parameters\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Pruned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvj-iIsO5M6U",
        "outputId": "d4587df0-5608-4045-be4a-97e5fa05c4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text after pruning: Vienna is the capital of Austria and has a population of over 1.5 million people. It is also known as the “City of Vienna” because it is located on the banks of the Danube River and is surrounded on all sides\n"
          ]
        }
      ],
      "source": [
        "generated = get_output(prompt, model, tokenizer)\n",
        "print(f\"Generated text after pruning: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGzXMQrVTULv"
      },
      "source": [
        "**Observation:** the result is slightly different from what the original model produced.\n",
        "\n",
        "Looking at the model’s new structure: the `gate_proj` and `up_proj` layers have had their `out_features` reduced to 6554 from 8192. Consequently, the `down_proj` layer has its `in_features` adjusted to match the new size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATAiqZW30NYN",
        "outputId": "574f024e-fc4d-4110-ca60-92e32f1a2c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
            "          (down_proj): Linear(in_features=6554, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qEmvooZycx"
      },
      "source": [
        "## Upload the model to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Ll_kqe5QzO",
        "outputId": "50f1b475-d93c-4923-d63b-b95a1b477cae"
      },
      "source": [
        "### Save the Model Locally\n",
        "\n",
        "```\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# model.save_pretrained(output_dir)\n",
        "model.save_pretrained(output_dir, max_shard_size=\"200MB\", safe_serialization=False)\n",
        "\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}