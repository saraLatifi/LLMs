{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install docarray --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install langchain_huggingface --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install langchain-community --quiet"
      ],
      "metadata": {
        "id": "6WZT07PN7jjq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "_odC0u2V8nIM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "wlv5LKVsCM6q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** the model is loaded from *unsloth* but not *google*.\n",
        "\n",
        "**Unsloth** is an open-source project that provides **extremely memory-efficient** and **fast loading & fine-tuning** of large language models, especially for LoRA and QLoRA training.\n",
        "- It is not from Google — it is developed independently. **Unsloth** is often chosen because its models load much lighter and train much faster."
      ],
      "metadata": {
        "id": "CjIlKJmzCYFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/gemma-2-9b-it\""
      ],
      "metadata": {
        "id": "ptq7L-8WCP9e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557,
          "referenced_widgets": [
            "4dff69a6aafc41cb812a7e818f38f23d",
            "0d7ffbb8a760469eb34c786fdfbd6601",
            "5ebfee6ee66340559c57601cdaefb2bc",
            "ab95a28199c64aaf83055bc2ed6e9df1",
            "bb3d2636d34d49ebb44aa910c75b1a5a",
            "5a80cd67926d4d308b7ee4cb6f97e365",
            "0238143b584c4d568f8051fffa50d68e",
            "1fafbb3533554f6d9c778d839f882fbc",
            "9dd9210be9f54c94972f5b4d55734d27",
            "41688bc07e2f49d1a7a243818302b8e1",
            "9db93c35df954722b1b0092c00d44788",
            "8e1f11cee8ba4fb996711e1962a7626b",
            "46a46030280c4c0e89d99d956489447c",
            "adc494b86aa248ea92de6928ad051735",
            "0f65538143594d25bde0ef2b0dcb648f",
            "a46eb0636b284bd89543a444b0b69339",
            "bc23f69902874d76a9e5af0ccff367d0",
            "b01fa92cc8df4afb93ffb451c47f85b7",
            "17940df0352e46df87303349cc86f02e",
            "a1fed0da9dcb42c39de2fcb66d2ae272",
            "82e4713578ec4f9599cf349646384e44",
            "20ad0f0aed1a4206af0a63b6ef713bdb",
            "0ad6313fc01e4d5db530306331833dd2",
            "e27014bbe8624c8e884bbf773a6efa3a",
            "71b488abf45649e9b8b8e4665af5915e",
            "d36842508b1a4cf78d87792739a5b1da",
            "e3ff46d269304bec8d14c755e8f232bc",
            "5a3f2e33876f4b738ce9cd71d6d52429",
            "9e4e13d76b9b44938196d2109f632573",
            "4c6e59bb554a433683089acedab672f1",
            "544682ecae9b4e82b079719e7a04ebed",
            "251a47ecdda34c8c9bbd221508b13d93",
            "3e923cee26ff4bef9640e0cea4b9fae9",
            "a808b3842e234de1a6a8944e8dd2f178",
            "454f94c2d3614034b14a3047ccbcf31e",
            "209b8b8d423d487caac0bc18e36ac584",
            "1219bdedfc9f407ba77b47718f04fcac",
            "30cef2962290433d892695f6880d8cc3",
            "d3e0969fc7444d6e8cd269a478c78648",
            "877f5d246cdf4ea898b9e88235e97bfd",
            "2ec1d26872c7411281dd32158f8dd759",
            "18066179fbaa4a0baedb28f94ef4794e",
            "3b897194b5f04702917f7df48a34e420",
            "21493e88f8984292b07d131910bc5f57",
            "33c711ac7f9441f3810ac19de4e98f3f",
            "37e709820c6d42ddb7687cd90855b480",
            "02f1b54f827f49eda47f8cf732ba15f4",
            "e88b8307810647d297623b00560bf6f7",
            "95f4a1cdfcaf4cf48c8549d656144e88",
            "82916887dce84bc7aa7cc8c8c5f66e29",
            "22c139c09e9d4c75ad6dcdca2a4ee255",
            "e9e8c73325a640b4831b2d7408473023",
            "e4e18d86bb31476b985bf6b980da9813",
            "72a597324ddd4266bcdb28d8cfe82163",
            "5713410fc76a4073948ec182c8972378",
            "c4210f641bc04204a96926abc023a634",
            "e9c1703878a54c86bb2982c2746ee230",
            "b901a5437d8141ec9a2873ef2f70d86d",
            "70d5d65cf23a40179e1dbbc4716f3483",
            "5836b276e14b42849f80f8f492aad5c0",
            "e803b5a2a75047b6b86aac062cba2b10",
            "59483d55eb8046c5a003c2454707f096",
            "f369cccccec14e9fab6a537222dfe0dd",
            "133376ce3431416a80996b3bfa917e04",
            "8a740b8b620f4f9cb7a251a3c53a5638",
            "a37b2108576f4021891a5c93527fc9dd",
            "e149d97279504f63bdc0bb78cd7f436f",
            "4968d6442f214c0fa68f9a76d83dbe22",
            "4b8adc63f9ce41fe9fe9baf93e710574",
            "d4d34bce4d7f40cb9c083917e13b412a",
            "03057d5d92314a73ab9a37d32f13a787",
            "727c593d596f4bf3b9838e312e27e466",
            "a97933fe205f49afb7da24c26df61f3e",
            "2db62d0e1e7343eaa07cda8f72974d07",
            "d6192a0ce58045298c0996576633243d",
            "4e8d8560ef474ef1b24bf1a20aa7192c",
            "1f5bacd06f2c4dbc8ecb3bf9835c8d7c",
            "30d682b7d25b44deac96a361a504f0e1",
            "7327b05804fb430898e0c7196b3712c4",
            "7036aebf8a7240009fa90da0af74b155",
            "3c935ee1e407457b92552e818750efc3",
            "0ecaea10db944f9abdd3aab645190b43",
            "bccfdd11cce44ba1ae08f9c37c760295",
            "36e5f25c8e7549a9ad0ad16d3ee2c868",
            "2e2e0f304289446e87fe1abadb9bfd3c",
            "705759e631404155a6f5410643c73b94",
            "bbff1463971a43ea925ff257a21a9a86",
            "0c69e118723243bc95cac937aa243f7c",
            "c8f0c0267c8349a2a14317a099bdf872",
            "a586565f0ba746a1a1cf9cfbc8ecf4a5",
            "1e60d887587c4a9ab2b57a0adda58265",
            "a5ae184d8095469e9614271f77cc54a0",
            "40b2ee45212f44ccbe9c8a084fc73998",
            "5adc02fab0354587b2aec8c0552a83da",
            "489d394967804248898878670e045d5c",
            "f4a44f1db8104e3896afe7a68d83fbf8",
            "4f0fa1a4d37d400d9bf47187e8f7a72a",
            "d38014bcaebb46099152728ee2c4cd4d",
            "06886bad93c0482fb141690e5747780c",
            "5827e49bf6b747d0ab6edab46168d58a",
            "97f4ec5a88fa41acae7d7ae15023fc28",
            "1032953083d0442296d1ec70ecfde6bc",
            "208c146e30bb4cfe93d3b5da0158ad2a",
            "b347554a5b364ccda0cb67e13eda1ea4",
            "a3ee5d66f5024096bb1e6620648093c5",
            "fbca7d16ec1f4105bc508b88800ef666",
            "11695f5428a842b7b71b953f9eaabcba",
            "462a81f300294d719ebc0c6d43466bc3",
            "633fdfd8bd8d474b976ffb3d544e533f",
            "ddea6ffc4a7a4753b085d60e0b2ce446",
            "447a598a7a5041e489f2709b633737ee",
            "4432f6ff22f348049f1d4920f5a3fe37",
            "e7f2c7175e294b8f9cf2c8911609ff25",
            "0c2d283d1d274183868be7b98a68e28b",
            "7803129e20964f9b90e691cd804b9f23",
            "47633e395e844d039db5e9704c2f81c0",
            "cb083e5a12fa4d97a63f07473974822e",
            "29907012eca74cb79cffde3bcf9fc327",
            "8fccd6866be443699157ce1a139f1e17",
            "12e6a125cfe54f3d8de06ad884d25973",
            "8831ca2084eb4098a405a5af1d08319a",
            "873f12a50496452bbd96f492d318d2f2",
            "5a828439a9494af489c7ac15ac202147",
            "95b639cd10df4654af1e5964e4ae44b7",
            "6886ee5bacda4e3a88b5bc8933ae25b3",
            "bde8d80c75ce4d70bd52fd8e419b8cb4",
            "60e4caf820d148e9a00340fe34720e67",
            "448328ab337f4436bc23862a45fe9b11",
            "6ea773f2f2dc4131bb4c1de25015f95a",
            "1e2bd6b7ad204f1abd36386c3a502826",
            "298564c13dd14e9b85db3f8f326b0a39",
            "6b17519acb824934af6ba86d29e8a7ed",
            "8380a8dddb4c42b49ba45ffd219b8b33",
            "4d065c4bb0564acb81ce2112f050221e",
            "029fce20c6d14daead71c5c739f1e032",
            "efdbe107030f4f9cb672aab96a607da8",
            "8708dee7dee84441b60d17514bee5b37",
            "6c3eb12c46c84f1a82b64451ac465146",
            "78ebda990468409c862c0c9c99f3a006",
            "99bf8562859846208cdc343ab834c216",
            "3eb808f6d5364917906d7a2c9b4b780b",
            "74eb5f13759b4aa1b794fcbef6900dcd",
            "38f3d80863554004a7f4a39379a0a0de"
          ]
        },
        "id": "u_vp6Yk9DpYo",
        "outputId": "30e6c184-625f-4ff3-f02c-4829a65f08e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dff69a6aafc41cb812a7e818f38f23d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e1f11cee8ba4fb996711e1962a7626b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ad6313fc01e4d5db530306331833dd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a808b3842e234de1a6a8944e8dd2f178"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c711ac7f9441f3810ac19de4e98f3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4210f641bc04204a96926abc023a634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e149d97279504f63bdc0bb78cd7f436f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d682b7d25b44deac96a361a504f0e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f0c0267c8349a2a14317a099bdf872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5827e49bf6b747d0ab6edab46168d58a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "447a598a7a5041e489f2709b633737ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "873f12a50496452bbd96f492d318d2f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8380a8dddb4c42b49ba45ffd219b8b33"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=64\n",
        ")"
      ],
      "metadata": {
        "id": "-HP97Hn9twCP",
        "outputId": "19fc5303-f343-41f9-d46d-1f51fc83c5ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "flk-ozsFE_AJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_name ='sentence-transformers/all-mpnet-base-v2'"
      ],
      "metadata": {
        "id": "0z9l8uC0E6T7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`embeddings` will be used by the **retriever**."
      ],
      "metadata": {
        "id": "zALNLXUfFS4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_name,\n",
        "    # model_kwargs={\"trust_remote_code\": True},\n",
        ")"
      ],
      "metadata": {
        "id": "oe6C0FOsmEFL",
        "outputId": "8ab972f5-d5dd-404e-8fd8-2f1b83bad3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "9cf1fec3103a44a9be485eff542eff03",
            "9f5aed25a0804f8494cd43e1d50b3f82",
            "1050a3f226b34482891a4c9d67a50a3a",
            "7480d71883c547008381f3f335cbcc53",
            "b3765777d83648028d7bc3505eb3f620",
            "695bdc8342784400aa0bd40a3085de1a",
            "4a156e58fc43418295fcac7e164a705a",
            "ad0a242afeea45e38ccb70f407027715",
            "73522bac89864b508d593174ef8aaeee",
            "ed7f589f0b644c3dad17265d35a733b3",
            "730500be54574a2ea4f221d7321d21cf",
            "3bbd9543c7694df88e9df3a499776437",
            "78f4defb8d0c477f81e1f9541ff4d376",
            "074aba52c11c4b20861cb744d154fbc1",
            "bc102e366ba34b68b760788c03581332",
            "d0c117dd16bf4ad099708b835e22adf3",
            "ada99a59c5644544ab6d0a3c682ad071",
            "e048b75babd447f8ac4e83b9ea9db10f",
            "6f191f6cf4af4dc692997561c0020796",
            "319f635e41fe4b5e90ea531858d9a5f6",
            "4563d3bdd0104ee2b7193d5b5476c236",
            "0c9a55bb3c184f29b8abcb4d0ff9e11d",
            "b95850e8c6764a3db6fafaa22a7db65a",
            "a321b5b04fef40689b404935bdb6d5a6",
            "14a7f3a3b4cc4378b40d81762f14c3cd",
            "d158777180994d44b55a9d1412c45dff",
            "f75b23283b2446fdb0ab5310c580a107",
            "1a78d0ed75a94ecaa074061dda00e426",
            "b8cd4a7da57843e4a75111bfe4ce7ee8",
            "c2aa085605284f88afdf078aea2df42c",
            "f98efeae57f048b2b2ba33d85e452ee4",
            "516b8bde8e0746a1abe1efd491c0683a",
            "cdda5a3ee8334ec39836d55e88b293a5",
            "7cc6094b392044f68c3ea23ee081a13c",
            "8a91375f19974e21a6c3d0787be4ba48",
            "4d358500aa20492b9a33ceb9cabd7119",
            "e51081d545324beebdfcf6d8704fd126",
            "824eb370c8c14d0c800eed3d6672ebad",
            "090a94d084a14ef3a9eda1e2df1b23d4",
            "7e09d41c2cf94e4daa12cea659b07239",
            "7d1bb095407e45bf9db258eea2f6822f",
            "3bd81aac495246298e01c93025a30311",
            "e26d239921d7454abab74a7bfd585cd2",
            "e238176041e64d6dbc19a41445efcb60",
            "802efc80a0494fb9b7bdbe30c836f80d",
            "102371d3bb7e4ebc9aa8837f5fa06b5f",
            "59accce1c323469fbb9fc091980ce05b",
            "b4e00f9b0d5f48a5b1a8f9d0b17b3463",
            "2b8586c34abd4f2796314f2f642d4589",
            "dffc00ed63ea450e944d7a70921415e7",
            "185256e937ad400d8c5121c7cf4b80fb",
            "559048463bab42f0af609cc52f983e6a",
            "800e85d74a414a74b7e6aaeed27a969c",
            "76deb04c90ef4da5a518e3b7d083e05c",
            "a66e6d3ace634bac98635fbe03dd066a",
            "27319d4166ad49dfbdc7d68a1647680b",
            "deba9514d99d4884b0216810fe8d7637",
            "a80a7267de6e450184e5bfd6297289eb",
            "5c4cc9da478a47fba61c90dda9f62076",
            "40354715b63d4bba8dc65e0f04447e6a",
            "cc82e7be4b3e4fb68bd0b09533233a28",
            "05c9e5189adc4551aa644f47005a0284",
            "c2c2613858ec446ea3b5eb10e924fd88",
            "08b7ed9fc8ab4dd19ffe7ac7e7fc8b38",
            "9d918ee1860f45c3bba40ebea7388708",
            "b8aa033924bb4799bba17926c62dc8e4",
            "bc6f29153d294ce48c288f3f163b197d",
            "a8c67a7460484b108831fb6b74cd00eb",
            "dae2cc5707a64599a615a77a761f70e9",
            "93d2d4244460443db461809d2846a71c",
            "93f72f4bcb0c4d3aa3794d4a3e70c4f4",
            "ec3065ceaa8c42939418cc128a7ce762",
            "c2b8d08a9cbf4ba987b052e3f27b9e6c",
            "6244a52c84b4406eac4afc7b987cc434",
            "2f8f276381384f17980a5b79ee689009",
            "9e9db1ae766a4244a6cda5156b7a92ea",
            "5ef809405f4d4c2cb7d760bd221894a6",
            "3c51c885693b4b20946bfb4559183b02",
            "bdcd10d3778b42c9973e02fcc6b4d721",
            "5647bb713f934549a321c5b0418f8de3",
            "f70e8030cebe4460abd10b61da1d0e4d",
            "2dee60497ba847d8b39392c38b5b5ac0",
            "e0ab46fc8d9b4a5dbc6ea482a4ca9f9a",
            "7b6fe9bb7b934dd1afbdacd6b675f478",
            "c2d774b68c17453ca253ef81aebbcc18",
            "28ad773f08ed4f1aaa305f76a58871a0",
            "1d65f80249d640e2992acbbf837ac196",
            "f8715b815c3b4ab6a816e1db80c4f66e",
            "cc8e0844d50448d4a174c3c014efc05a",
            "ad9c61b9c36142f69dc5a3784e91a97c",
            "c60fabfd361943a896c103fb27463a86",
            "b67c960d9d4c4ad9b350e3ffaef06d26",
            "8722ee0214c1480cb726a8b75cea9440",
            "121a4b357ae840d0892ad017635be31b",
            "a3bc85e1ca8f4cb78011c09d426d18d1",
            "a8a34e18ae004dd2bee9375f53c31109",
            "7292bc96af664ad087891532eaaa3fb8",
            "4888a539eb3348229fd64166dda7a76e",
            "208365e57a9d4145a7a0bd4c3f86645f",
            "e7bd85d21b684ee3b8abc582500310a9",
            "7751de0f95f0452692af0f7f4bdfdf29",
            "81e04f80352a4e5f9309cd3e333b42ce",
            "f6c9636841bc4c88ada3906853430715",
            "52c84181a947421aa51235f679388399",
            "362ed490fe7643b2bd93ecb898d11e11",
            "024135ea82a34c49a9800785d20a3bd8",
            "281f7d5843484f97b160ea48d02f8056",
            "10143a7961fd402e82c0f4f6dbf27bd4",
            "0740725ceb6348148039b20b9ea12371",
            "f8e54cddc30f4945bb566882776fb01b",
            "32461644c42749abbf5d3f852b2a4a99",
            "08bf8d3323f94cd8ac357c1355df6774",
            "f460e4fb2a7a4cc09c5019e94d6fd479",
            "a793a68b043f4f99bbc41a79e753beda",
            "8ea653dcb2bd4114a9675325b07540ba",
            "e377f0fdfb7e4b40a1a0da27dd26a2b1",
            "23a12d51614740419fa0731a17dd198a",
            "e1fc4d069ae247108db9532a06f20c89",
            "b60d51f111c94fdd8252b8918c571e5f",
            "9e3972453e3749a89f109410cbc518b3",
            "242d2d57194e417e8ab5896ebae4e596"
          ]
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cf1fec3103a44a9be485eff542eff03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bbd9543c7694df88e9df3a499776437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b95850e8c6764a3db6fafaa22a7db65a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cc6094b392044f68c3ea23ee081a13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "802efc80a0494fb9b7bdbe30c836f80d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27319d4166ad49dfbdc7d68a1647680b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc6f29153d294ce48c288f3f163b197d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c51c885693b4b20946bfb4559183b02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc8e0844d50448d4a174c3c014efc05a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7bd85d21b684ee3b8abc582500310a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32461644c42749abbf5d3f852b2a4a99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covert **transformers'** `pipeline` to `HuggingFacePipeline`."
      ],
      "metadata": {
        "id": "45Ff6z9-Fo4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "9cSWVN0SHOy9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f6WOmapugVVo"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=text_gen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_chat_template_and_response(prompt):\n",
        "    messages = [\n",
        "    {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False  # as reasoning is not needed\n",
        "    )\n",
        "\n",
        "    # invoke(): is needed to get the output form HuggingFacePipeline\n",
        "    # replace(): causal models genetaye the question along with answer => we remove the question from the generated text\n",
        "    return llm.invoke(text).replace(text, '')"
      ],
      "metadata": {
        "id": "RQnjFIRwH2mY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test: Get the Output"
      ],
      "metadata": {
        "id": "rI9FnOV_SWQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formt 1\n",
        "To get the output in `HuggingFacePipeline` format:"
      ],
      "metadata": {
        "id": "fnPS0GBhJZXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aKyNd40qgVVq",
        "outputId": "6636d7e8-0d1a-47b3-efc2-76d7e8fc5218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind.\n",
            "\n",
            "Here are some key things to know about me:\n",
            "\n",
            "* **Open-weights:** My weights are publicly accessible, meaning anyone can see and use the underlying code that makes me work. This promotes transparency\n"
          ]
        }
      ],
      "source": [
        "response = apply_chat_template_and_response(\"Who are you?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format 2\n",
        "To get the output in the **LangChain** standard format:"
      ],
      "metadata": {
        "id": "qVC2pMZGOCM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "VjQatb3TOTO6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jH06VO3kgVVq",
        "outputId": "84cf94f7-b501-43ed-c122-a29efd8d0298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind.\n",
            "\n",
            "Here are some key things to know about me:\n",
            "\n",
            "* **Open-Weights:** My weights are publicly accessible. This means anyone can see and use the underlying code that makes me work.\n",
            "*\n"
          ]
        }
      ],
      "source": [
        "parser = StrOutputParser()\n",
        "response_from_model = apply_chat_template_and_response(\"Who are you?\")\n",
        "parsed_response = parser.parse(response_from_model)\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GRrhZ4vegVVq"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a helpful and knowledgeable AI assistant. Use only the information retrieved from the documents to answer the user's question in English.\n",
        "If the answer is not found in the retrieved context, respond with: \"Sorry, I don't have that information!\" Do not use your own knowledge beyond the provided context.\n",
        "Be accurate, clear, and polite. Never mention the documents or the retrieval process in your response.\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "l9E0kRmsTEhX",
        "outputId": "8f410033-8f27-4a33-d144-94f8d37b6732"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYou are a helpful and knowledgeable AI assistant. Use only the information retrieved from the documents to answer the user\\'s question in English.\\nIf the answer is not found in the retrieved context, respond with: \"Sorry, I don\\'t have that information!\" Do not use your own knowledge beyond the provided context.\\nBe accurate, clear, and polite. Never mention the documents or the retrieval process in your response. \\nContext: Here is some context\\n\\nQuestion: Here is a question\\n\\nAnswer:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Response (After Giving the Context to LLM)"
      ],
      "metadata": {
        "id": "nULpo2wNUQSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"I am Sara, and I work as an ML engineer.\""
      ],
      "metadata": {
        "id": "ncf_BkJ5U_Mu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MfhbsTk_gVVq",
        "outputId": "ee54e98b-a4c9-4e77-ed9b-f1e700420ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a helpful and knowledgeable AI assistant.  \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(context = context, question=\"Who are you?\")\n",
        "response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "parsed_response = parser.parse(response_from_model)\n",
        "print(parsed_response.replace(formatted_prompt, \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NFKMvtX0gVVq",
        "outputId": "12221a56-a97f-42f7-f8ba-a9b54c8d7b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I don't have that information! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(context = context, question=\"How old are you?\")\n",
        "response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "parsed_response = parser.parse(response_from_model)\n",
        "print(parsed_response.replace(formatted_prompt, \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt = prompt.format(context = context, question=\"Who am I?\")\n",
        "response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "parsed_response = parser.parse(response_from_model)\n",
        "print(parsed_response.replace(formatted_prompt, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKhPm5eqVTB7",
        "outputId": "07a65bcb-2697-4081-e632-67478adfc0af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are Sara, and you work as an ML engineer. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt = prompt.format(context = context, question=\"How old am I?\")\n",
        "response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "parsed_response = parser.parse(response_from_model)\n",
        "print(parsed_response.replace(formatted_prompt, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH1yMBGeVX-l",
        "outputId": "fcd13b51-1e56-4ebc-fecd-e35f1f5054b6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I don't have that information! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context as PDF"
      ],
      "metadata": {
        "id": "kv_VwrXpYoxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O sample_doc.pdf \"https://github.com/saraLatifi/LLMs/raw/main/rag/Agentic_AI.pdf\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wdch2NhyBhE",
        "outputId": "92a48599-801e-4ae8-c47e-6605c36fc00e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-12 15:07:24--  https://github.com/saraLatifi/LLMs/raw/main/rag/Agentic_AI.pdf\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/saraLatifi/LLMs/main/rag/Agentic_AI.pdf [following]\n",
            "--2025-12-12 15:07:24--  https://raw.githubusercontent.com/saraLatifi/LLMs/main/rag/Agentic_AI.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3921 (3.8K) [application/octet-stream]\n",
            "Saving to: ‘sample_doc.pdf’\n",
            "\n",
            "sample_doc.pdf      100%[===================>]   3.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-12 15:07:24 (38.0 MB/s) - ‘sample_doc.pdf’ saved [3921/3921]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Loader for LangChain"
      ],
      "metadata": {
        "id": "KSdyhzO4oRgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "cDBGoBF0Y75q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "is_mM6rTgVVr",
        "outputId": "39d3e165-d005-4ea6-b2da-16ff4e9157a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Agentic AI\\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\\nOverview\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S.\\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user\\'s prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\\nMemory systems for agents include Mem0, MemGPT, and MemOS.\\nHistory\\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\\nTraining and testing\\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man\\'s Sky as well as replicas of company websites, have also been used for training AI agents.\\nArchitectural patterns\\nCommon architectural design patterns for agents include:\\n1) Retrieval-augmented generation (RAG)\\n2) ReAct (Reason + Act), an extension of chain-of-thought prompting that queries the underlying model to explain its reasoning before taking any action\\n3) Reflexion, which uses an LLM to create feedback on the agent\\'s plan of action and stores that feedback in a memory cache\\n4) A tool/agent registry, for organizing software functions or other agents that the agent can use\\n5) One-shot model querying, which queries the model once to create the plan of action')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "loader = PyPDFLoader(\"sample_doc.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "#pages = loader.load()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-community langchain-core"
      ],
      "metadata": {
        "id": "nEVKhIOUGxBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "Zc8zMyItHzQQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set `chunk_size` based on the **context window** of the LLM model."
      ],
      "metadata": {
        "id": "q9C_FIlv6yrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)"
      ],
      "metadata": {
        "id": "VivhgdIh5wFs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_documents = text_splitter.split_documents(pages)[:5]"
      ],
      "metadata": {
        "id": "qfgYRUAg6FRc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# this part of the document is used to retrieve relevant information"
      ],
      "metadata": {
        "id": "WNa_JE4eDuMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_Tr8Y3H-mHt",
        "outputId": "85091161-e99a-4237-f650-c4fb99db6b55"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Agentic AI\\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\\nOverview\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S.'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"Researchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S.\\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\"),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Companies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\\nMemory systems for agents include Mem0, MemGPT, and MemOS.\\nHistory'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Memory systems for agents include Mem0, MemGPT, and MemOS.\\nHistory\\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\\nTraining and testing\\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man\\'s Sky as well as replicas of company websites, have also been used for training AI agents.\\nArchitectural patterns\\nCommon architectural design patterns for agents include:\\n1) Retrieval-augmented generation (RAG)\\n2) ReAct (Reason + Act), an extension of chain-of-thought prompting that queries the underlying model to explain its reasoning before taking any action\\n3) Reflexion, which uses an LLM to create feedback on the agent\\'s plan of action and stores that feedback in a memory cache'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"3) Reflexion, which uses an LLM to create feedback on the agent's plan of action and stores that feedback in a memory cache\\n4) A tool/agent registry, for organizing software functions or other agents that the agent can use\\n5) One-shot model querying, which queries the model once to create the plan of action\")]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The external data source is a small PDF file, so a **vector store** is used—specifically, *DocArrayInMemorySearch*, since the document can be stored in RAM.\n",
        "\n",
        "For large external datasets, a **vector database** like *Faiss* can be used."
      ],
      "metadata": {
        "id": "Cl5SJ3oR9AEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Uc8_qHjygVVs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# it stores the embeddings of the text_documents\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "W-Yxe12xgVVs"
      },
      "outputs": [],
      "source": [
        "query = \"Who is Andrew Ng?\"\n",
        "# vectorstore will act as a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "# retrieves the related chunk\n",
        "retrieved_context = retriever.invoke(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqcXqktaIhVi",
        "outputId": "4b764e3f-079c-4b06-b8ac-99faba00f82a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Memory systems for agents include Mem0, MemGPT, and MemOS.\\nHistory\\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\\nTraining and testing\\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man\\'s Sky as well as replicas of company websites, have also been used for training AI agents.\\nArchitectural patterns\\nCommon architectural design patterns for agents include:\\n1) Retrieval-augmented generation (RAG)\\n2) ReAct (Reason + Act), an extension of chain-of-thought prompting that queries the underlying model to explain its reasoning before taking any action\\n3) Reflexion, which uses an LLM to create feedback on the agent\\'s plan of action and stores that feedback in a memory cache'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"Researchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S.\\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\"),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Companies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\\nMemory systems for agents include Mem0, MemGPT, and MemOS.\\nHistory'),\n",
              " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2025-12-12T14:31:08+01:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2025-12-12T14:31:08+01:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': 'sample_doc.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Agentic AI\\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\\nOverview\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S.')]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline"
      ],
      "metadata": {
        "id": "6wWlNIWxLqE9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZxpIUCH8gVVt",
        "outputId": "a7c5c2bd-4393-473f-bd3a-bdd86cb89156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is Andrew Ng?\n",
            "Answer: Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question: Explain AI agents.\n",
            "Answer: AI agents, also known as compound AI systems or agentic AI, are intelligent agents capable of operating autonomously in complex environments.  \n",
            "\n",
            "They prioritize decision-making over content creation and don't require continuous human supervision.  \n",
            "\n",
            "Key attributes of AI agents include:\n",
            "\n",
            "* Complex goal structures\n",
            "* Natural language interfaces\n",
            "\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"Who is Andrew Ng?\",\n",
        "    \"Explain AI agents.\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    ### RAG pipeline\n",
        "    # retrive the context from external data source\n",
        "    retrieved_context = retriever.invoke(question)\n",
        "    # add context to the prompt\n",
        "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
        "    # query the prompt & get the response\n",
        "    response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "    # parse the response\n",
        "    parsed_response = parser.parse(response_from_model)\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {parsed_response}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chatbot with RAG"
      ],
      "metadata": {
        "id": "Gi9njS7WMUIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample question: *Who used the term “agentic” for the first time?*"
      ],
      "metadata": {
        "id": "-yRG35AMMp9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "PcIYWAGcgVVt",
        "outputId": "bf058a9c-b2ef-49a5-ffde-481c498f5a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Say 'exit' or 'quit' to exit the loop\n",
            "User question: Hi\n",
            "Question: Hi\n",
            "Answer: Hello!  How can I help you? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Say 'exit' or 'quit' to exit the loop\n",
            "User question: Who used the term “agentic” for the first time?\n",
            "Question: Who used the term “agentic” for the first time?\n",
            "Answer: Andrew Ng.  \n",
            "\n",
            "\n",
            "Say 'exit' or 'quit' to exit the loop\n",
            "User question: exit\n",
            "Question: exit\n",
            "Exiting the conversation. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    print(\"Say 'exit' or 'quit' to exit the loop\")\n",
        "    question = input('User question: ')\n",
        "    print(f\"Question: {question}\")\n",
        "    if question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Exiting the conversation. Goodbye!\")\n",
        "        break\n",
        "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
        "    response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "    parsed_response = parser.parse(response_from_model)\n",
        "    print(f\"Answer: {parsed_response}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}