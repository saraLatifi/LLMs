{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PAL (program-aided language model)**"
      ],
      "metadata": {
        "id": "EKwaxNuskQ2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install bitsandbytes accelerate transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNGXGWZixER",
        "outputId": "9d5121b4-2eeb-44f9-dacd-15183d8f7262"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import re"
      ],
      "metadata": {
        "id": "qe5vDqlUmxvf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mistral-7b-instruct understands programming\n",
        "model_id = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "db766f9aaf0c45029fbd8f0b3ad39012",
            "100471702a3b4b5b95a2f1b7cb8cc244",
            "4b9e902176994907bc4899350a2282ea",
            "c51b7285874445f2b1a8161c4360302c",
            "81d98dcf3a43459f8cc3a79772ee96a3",
            "36bf84a0654c4ccca5ea24fc525f07da",
            "aa2ed4d20acc4ee195dca16168298700",
            "4ecf8e7859dd4c18a46494d588a0fb81",
            "3d8c4bc3d4064ebaa4937d16474c633a",
            "9183d8f035934a88b3af6a58c7cb5c39",
            "14bce24998414e239dc55ef46655d472"
          ]
        },
        "id": "Efkd2LURm2Ip",
        "outputId": "12567e8d-5697-42ac-e400-7a423d180a4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db766f9aaf0c45029fbd8f0b3ad39012"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)"
      ],
      "metadata": {
        "id": "AL5UEN7Mm4z5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question, observation=None):\n",
        "    prompt = f\"\"\"You are a helpful assistant that solves problems step by step using Python.\n",
        "Question: {question}\n",
        "Let's solve it step by step.\n",
        "\"\"\"\n",
        "    if observation:\n",
        "        prompt += f\"# Previous Observation: {observation}\\n\"\n",
        "    prompt += \"# Write a valid Python code, and store the final answer in a variable named 'result'.\\n\"\n",
        "    prompt += \"Thought:\\n# Think about the problem.\\nAction:\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "wTspf9vNm_bn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_code(text):\n",
        "  \"\"\"\n",
        "  To extract the code from the generated text\n",
        "  \"\"\"\n",
        "  match = re.search(r\"Action:\\n([\\s\\S]+)\", text)\n",
        "  if match:\n",
        "      code = match.group(1).strip()\n",
        "      lines = code.splitlines()\n",
        "      # ignore comments\n",
        "      code_lines = [line for line in lines if line.strip() and not line.strip().startswith(\"#\")]\n",
        "      return \"\\n\".join(code_lines)\n",
        "  return \"\""
      ],
      "metadata": {
        "id": "0cVJDj3onC7t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_code(code):\n",
        "    exec_env = {}\n",
        "    try:\n",
        "        exec(code, {}, exec_env)\n",
        "    except Exception as e:\n",
        "        print(\"Execution error:\", e)\n",
        "        return f\"Execution Error: {e}\"\n",
        "    if \"result\" in exec_env:\n",
        "        return exec_env[\"result\"]\n",
        "    \"\"\"\n",
        "    to give a chance to get the result when\n",
        "    the LLM does not stores the final result in the 'result' varibale:\n",
        "    return the last generated number\n",
        "    \"\"\"\n",
        "    nums = [v for v in exec_env.values() if isinstance(v, (int, float))]\n",
        "    if nums:\n",
        "        return nums[-1]\n",
        "    return \"No result variable found.\"\n"
      ],
      "metadata": {
        "id": "nUJ_ideunF-a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pal_chain(question, max_steps=3):\n",
        "    observation = None\n",
        "    # give the model 3 chance to calculate the final result\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n--- Step {step} ---\")\n",
        "        prompt = create_prompt(question, observation)\n",
        "        output = llm(prompt)[0]\n",
        "        generated = output.get('generated_text', '') or output.get('text', '')\n",
        "        code = extract_code(generated)\n",
        "        if not code:\n",
        "            print(\"No code extracted.\")\n",
        "            return \"No code generated.\"\n",
        "        print(\"Generated code:\\n\", code)\n",
        "        observation = run_code(code)\n",
        "        print(\"Observation:\", observation)\n",
        "        # if a number is generated, stop trying!\n",
        "        if isinstance(observation, (int, float, str)):\n",
        "            break\n",
        "    return observation\n"
      ],
      "metadata": {
        "id": "_P-yUhBEkZfE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"Peppa has 5 candies. She buys 2 more packs of candies.\n",
        " Each pack has 3 candies. How many candies does she have now?\"\"\"\n",
        "\n",
        "final_answer = pal_chain(question)\n",
        "print(\"\\n✅ Final Answer:\", final_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjpAo2IEnJzn",
        "outputId": "1dcebd5a-0d4d-4f29-83f4-abebb52c5ee3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=200) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 1 ---\n",
            "Generated code:\n",
            " initial_candies = 5\n",
            "new_candies = 2\n",
            "candies_per_pack = 3\n",
            "new_candies_count = new_candies * candies_per_pack\n",
            "result = initial_candies + new_candies_count\n",
            "print(f\"Peppa has {result} candies now.\")\n",
            "result = initial_candies + new_candies_count\n",
            "print(f\"Peppa has {result} candies now.\")\n",
            "Peppa has 11 candies now.\n",
            "Peppa has 11 candies now.\n",
            "Observation: 11\n",
            "\n",
            "✅ Final Answer: 11\n"
          ]
        }
      ]
    }
  ]
}